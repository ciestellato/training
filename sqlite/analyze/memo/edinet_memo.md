# EDINET APIã§è‡ªåˆ†ã ã‘ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ãŸã„ï¼<!-- omit in toc -->

- [å‚è€ƒURL](#å‚è€ƒurl)
- [ä½œæ¥­ãƒ­ã‚°](#ä½œæ¥­ãƒ­ã‚°)
  - [EDINET APIæ¥ç¶šå®Œäº† (2025-09-17)](#edinet-apiæ¥ç¶šå®Œäº†-2025-09-17)
    - [1. APIã‚­ãƒ¼ã®ç™ºè¡Œ](#1-apiã‚­ãƒ¼ã®ç™ºè¡Œ)
    - [2. EDINET APIã§æƒ…å ±ã‚’å–å¾—ã™ã‚‹](#2-edinet-apiã§æƒ…å ±ã‚’å–å¾—ã™ã‚‹)
      - [ç’°å¢ƒæ§‹ç¯‰](#ç’°å¢ƒæ§‹ç¯‰)
      - [ã‚³ãƒ¼ãƒ‰](#ã‚³ãƒ¼ãƒ‰)
        - [edinet\_config.py](#edinet_configpy)
        - [edinet\_steps.py](#edinet_stepspy)
        - [edinet\_main.py](#edinet_mainpy)
      - [çµæœ](#çµæœ)
- [é€²æ—ãƒ­ã‚°ã®CSVå‡ºåŠ›(2025-09-18)](#é€²æ—ãƒ­ã‚°ã®csvå‡ºåŠ›2025-09-18)
- [ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãƒªã‚«ãƒãƒªãƒ¼(2025-09-18)](#ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãƒªã‚«ãƒãƒªãƒ¼2025-09-18)
- [Zipã®ä¸­èº«ç¢ºèª(2025-09-18)](#zipã®ä¸­èº«ç¢ºèª2025-09-18)

## å‚è€ƒURL

1. [é‡‘èåº EDINET é–²è¦§ã‚µã‚¤ãƒˆ](https://disclosure2.edinet-fsa.go.jp/week0010.aspx)
2. [[EDINET] ä¸Šå ´ä¼æ¥­ã®æ¥­ç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’APIã§å–å¾—ã™ã‚‹](https://qiita.com/hifistar/items/0114c6f60ded96785178)
3. [ã€PythonÃ—EDINET APIã€‘CSVã§è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ï¼å·®åˆ†æ›´æ–°ã§æ§‹ç¯‰ã™ã‚‹é‡‘èåˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹](https://qiita.com/invest-aitech/items/7e13e89821bd754dfc25)


## ä½œæ¥­ãƒ­ã‚°

### EDINET APIæ¥ç¶šå®Œäº† (2025-09-17)

#### 1. APIã‚­ãƒ¼ã®ç™ºè¡Œ

å¿…è¦äº‹é …ã‚’å…¥åŠ›ã—ã¦ã€APIã‚­ãƒ¼ã‚’ç™ºè¡Œã™ã‚‹ã€‚

[EDINET API v2 ä»•æ§˜æ›¸](https://disclosure2dl.edinet-fsa.go.jp/guide/static/disclosure/download/ESE140206.pdf)

> 2-3-1 ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ä½œæˆã«ã¤ã„ã¦ 
> 
> EDINET API ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚
>  
> 2-3-1-1 ã‚µã‚¤ãƒ³ã‚¤ãƒ³ç”»é¢ã®è¡¨ç¤º 
> 
> ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹å ´åˆã¯ã€æ¬¡ã®URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ã‚µã‚¤ãƒ³ã‚¤ãƒ³ç”»é¢ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ 
> 
> https://api.edinet-fsa.go.jp/api/auth/index.aspx?mode=1

â€»ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£é™¤ã—ã¦ã„ãªã„ã¨APIã‚­ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„ã®ã§æ³¨æ„

#### 2. EDINET APIã§æƒ…å ±ã‚’å–å¾—ã™ã‚‹

å‚è€ƒURL3ã®Qiitaã®è¨˜äº‹ã¯Google Colaboratoryã¨Google Driveã ã£ãŸãŸã‚ã€ã¾ãšã¯ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ãã€‚

##### ç’°å¢ƒæ§‹ç¯‰

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸã‚‚ã®(ã™ã¹ã¦pipã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)

- requests
- pandas
- tqdm

`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’ä¿å­˜ã—ã¦ãŠã

##### ã‚³ãƒ¼ãƒ‰

Copilotã«ç›¸è«‡ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚

###### edinet_config.py

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm  # notebookã§ã¯ãªãé€šå¸¸ç‰ˆ
import warnings
from dotenv import load_dotenv  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãŠãï¼‰
env_path = Path(__file__).resolve().parents[2] / ".env"
load_dotenv(dotenv_path=env_path)

# --- è¨­å®šé …ç›® ---
class Config:
    """EDINETãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®è¨­å®šã‚¯ãƒ©ã‚¹"""
    # ã‚¢ã‚¯ã‚»ã‚¹è¨­å®š
    API_BASE_URL = "https://disclosure.edinet-fsa.go.jp/api/v2"
    REQUEST_TIMEOUT = 30
    DOWNLOAD_TIMEOUT = 60

    # åŸºæœ¬ãƒ‘ã‚¹ï¼ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    BASE_DIR = Path(os.getenv("EDINET_BASE_DIR", "C:/Users/0602JP/Documents/EDINET_DB/"))
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆ
    SAVE_FOLDER = BASE_DIR / "01_zip_files/"

    # APIã‚­ãƒ¼ã®å–å¾—ã¨ãƒã‚§ãƒƒã‚¯
    API_KEY = os.getenv("EDINET_API_KEY")
    if not API_KEY:
        raise ValueError("EDINET_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ã‚’æ‹…ä¿ã™ã‚‹ãŸã‚ã€ä½•æ—¥åˆ†é¡ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ã™ã‚‹ã‹
    RELIABILITY_DAYS = 7
    # åˆå›å®Ÿè¡Œæ™‚ã«ä½•å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹
    INITIAL_FETCH_YEARS = 1

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰
    # 120: æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸, 140: å››åŠæœŸå ±å‘Šæ›¸, 160: åŠæœŸå ±å‘Šæ›¸
    TARGET_DOC_TYPE_CODES = ['120', '140', '160']
```

###### edinet_steps.py

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False,
                timeout=Config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            res_json = response.json()
            if res_json.get('results'):
                new_docs.extend(res_json['results'])
        except requests.exceptions.RequestException as e:
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— - {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1)

    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        summary = pd.concat([summary, temp_df], ignore_index=True)

    summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
    summary.dropna(subset=['docID'], inplace=True)
    summary = summary.drop_duplicates(subset='docID', keep='last')
    summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)
    try:
        summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
        logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    return summary

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and "
        "secCode.notna() and secCode != 'None' and "
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}"
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("\nğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']

        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        if pd.isna(submit_date):
            target_folder = Config.SAVE_FOLDER / "unknown_date"
        else:
            year = submit_date.year
            quarter = (submit_date.month - 1) // 3 + 1
            target_folder = Config.SAVE_FOLDER / str(year) / f"Q{quarter}"

        target_folder.mkdir(parents=True, exist_ok=True)
        zip_path = target_folder / f"{doc_id}.zip"

        # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
        params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

        try:
            r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
            r.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        except requests.exceptions.RequestException as e:
            logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
            logging.debug(traceback.format_exc())
            if zip_path.exists():
                zip_path.unlink()
        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)
```

###### edinet_main.py

```
from edinet_steps import (
    step1_create_and_summarize,
    step2_check_download_status,
    step3_execute_download
)
import logging
import traceback
from edinet_config import Config
from pathlib import Path

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆã‚’æŒ‡å®š
log_path = Config.BASE_DIR / "edinet_log.txt"

# ãƒ­ã‚°è¨­å®šï¼šINFOä»¥ä¸Šã‚’è¡¨ç¤ºã€ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä¸¡æ–¹ã«å‡ºåŠ›
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def main() -> bool:
    """EDINETãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    try:
        logging.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        summary_data = step1_create_and_summarize()

        if summary_data.empty:
            logging.warning("ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            return False

        files_to_download = step2_check_download_status(summary_data)

        if files_to_download.empty:
            logging.info("æ–°è¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            step3_execute_download(files_to_download)

        logging.info("å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ğŸ‰")
        return True

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return False

if __name__ == '__main__':
    success = main()
    if not success:
        logging.warning("ä¸€éƒ¨ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
```

##### çµæœ

zipãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ãŸ

## é€²æ—ãƒ­ã‚°ã®CSVå‡ºåŠ›(2025-09-18)

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’CSVã§å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚

```
def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    log_records = []  # ãƒ­ã‚°è¨˜éŒ²ç”¨ãƒªã‚¹ãƒˆ

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        if pd.isna(submit_date):
            target_folder = Config.SAVE_FOLDER / "unknown_date"
        else:
            year = submit_date.year
            quarter = (submit_date.month - 1) // 3 + 1
            target_folder = Config.SAVE_FOLDER / str(year) / f"Q{quarter}"

        target_folder.mkdir(parents=True, exist_ok=True)
        zip_path = target_folder / f"{doc_id}.zip"

        # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
        params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

        try:
            r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
            r.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            status = "success"
            error_msg = ""
        except requests.exceptions.RequestException as e:
            logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
            logging.debug(traceback.format_exc())
            if zip_path.exists():
                zip_path.unlink()
            status = "failed"
            error_msg = str(e)

        log_records.append({
            "docID": doc_id,
            "submitDateTime": submit_date,
            "status": status,
            "errorMessage": error_msg,
            "savedPath": str(zip_path if status == "success" else ""),
            "timestamp": timestamp
        })

        time.sleep(0.1)
        
    # CSVã«ä¿å­˜
    log_df = pd.DataFrame(log_records)
    log_csv_path = Config.BASE_DIR / "download_log.csv"
    try:
        log_df.to_csv(log_csv_path, index=False, encoding='utf_8_sig')
        logging.info(f"ğŸ“„ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {log_csv_path}")
    except Exception as e:
        logging.error(f"ãƒ­ã‚°CSVã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)
```

## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãƒªã‚«ãƒãƒªãƒ¼(2025-09-18)

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’è¨˜éŒ²ã—ã¦ã€å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ä»•çµ„ã¿ã‚’ä½œã‚ŠãŸã„

- æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¨˜éŒ²å»ƒæ­¢
- å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¨˜éŒ²
- å¤±æ•—å±¥æ­´ã‹ã‚‰ã®å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã—ãŸã‚‰å¤±æ•—å±¥æ­´ã‹ã‚‰å‰Šé™¤
- mainãƒ¡ã‚½ãƒƒãƒ‰
  - å‰å›ã®å¤±æ•—ã®ãƒªãƒˆãƒ©ã‚¤
  - ä»Šå›ã®å‡¦ç†
  - ä»Šå›ã®å¤±æ•—ã®ãƒªãƒˆãƒ©ã‚¤

```
from edinet_steps import (
    step1_create_and_summarize,
    step2_check_download_status,
    step3_execute_download,
    retry_failed_downloads
)
import logging
import traceback
from edinet_config import Config
from pathlib import Path

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆã‚’æŒ‡å®š
log_path = Config.BASE_DIR / "edinet_log.txt"

# ãƒ­ã‚°è¨­å®šï¼šINFOä»¥ä¸Šã‚’è¡¨ç¤ºã€ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä¸¡æ–¹ã«å‡ºåŠ›
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def main() -> bool:
    """EDINETãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    try:
        logging.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        # ğŸ” Step 0: å‰å›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œ

        logging.info("ğŸ” Step 0: å‰å›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
        retry_failed_downloads()

        # ğŸ§¾ Step 1: ã‚µãƒãƒªãƒ¼ä½œæˆ

        summary_data = step1_create_and_summarize()
        if summary_data.empty:
            logging.warning("ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            return False

        # ğŸ“‹ Step 2: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æŠ½å‡º

        files_to_download = step2_check_download_status(summary_data)

        # ğŸ“¦ Step 3: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ

        if files_to_download.empty:
            logging.info("æ–°è¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            step3_execute_download(files_to_download)

        # ğŸ” Step 4: ä»Šå›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œ
        
        logging.info("ğŸ” Step 4: ä»Šå›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
        retry_failed_downloads()

        logging.info("å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ğŸ‰")
        return True

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return False

if __name__ == '__main__':
    success = main()
    if not success:
        logging.warning("ä¸€éƒ¨ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
```

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm  # notebookã§ã¯ãªãé€šå¸¸ç‰ˆ
import warnings
from dotenv import load_dotenv  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãŠãï¼‰
env_path = Path(__file__).resolve().parents[2] / ".env"
load_dotenv(dotenv_path=env_path)

# --- è¨­å®šé …ç›® ---
class Config:
    """EDINETãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®è¨­å®šã‚¯ãƒ©ã‚¹"""
    # ã‚¢ã‚¯ã‚»ã‚¹è¨­å®š
    API_BASE_URL = "https://disclosure.edinet-fsa.go.jp/api/v2"
    REQUEST_TIMEOUT = 30
    DOWNLOAD_TIMEOUT = 60

    # åŸºæœ¬ãƒ‘ã‚¹ï¼ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    BASE_DIR = Path(os.getenv("EDINET_BASE_DIR", "C:/Users/0602JP/Documents/EDINET_DB/"))
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆ
    SAVE_FOLDER = BASE_DIR / "01_zip_files/"
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ãƒ­ã‚°ã®ä¿å­˜å…ˆ
    FAILED_LOG_PATH = BASE_DIR / "failed_downloads.csv"

    # APIã‚­ãƒ¼ã®å–å¾—ã¨ãƒã‚§ãƒƒã‚¯
    API_KEY = os.getenv("EDINET_API_KEY")
    if not API_KEY:
        raise ValueError("EDINET_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ã‚’æ‹…ä¿ã™ã‚‹ãŸã‚ã€ä½•æ—¥åˆ†é¡ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ã™ã‚‹ã‹
    RELIABILITY_DAYS = 7
    # åˆå›å®Ÿè¡Œæ™‚ã«ä½•å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹
    INITIAL_FETCH_YEARS = 1

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰
    # 120: æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸, 140: å››åŠæœŸå ±å‘Šæ›¸, 160: åŠæœŸå ±å‘Šæ›¸
    TARGET_DOC_TYPE_CODES = ['120', '140', '160']
```

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False,
                timeout=Config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            res_json = response.json()
            if res_json.get('results'):
                new_docs.extend(res_json['results'])
        except requests.exceptions.RequestException as e:
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— - {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1)

    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        summary = pd.concat([summary, temp_df], ignore_index=True)

    summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
    summary.dropna(subset=['docID'], inplace=True)
    summary = summary.drop_duplicates(subset='docID', keep='last')
    summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)
    try:
        summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
        logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    return summary

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and "
        "secCode.notna() and secCode != 'None' and "
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}"
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("\nğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download

def download_single_file(doc_id: str, submit_date, save_folder: Path) -> bool:
    """1ä»¶ã®EDINETãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¿å­˜ã€‚æˆåŠŸãªã‚‰Trueã€å¤±æ•—ãªã‚‰False"""
    if pd.isna(submit_date):
        target_folder = save_folder / "unknown_date"
    else:
        year = submit_date.year
        quarter = (submit_date.month - 1) // 3 + 1
        target_folder = save_folder / str(year) / f"Q{quarter}"

    target_folder.mkdir(parents=True, exist_ok=True)
    zip_path = target_folder / f"{doc_id}.zip"

    # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
    params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

    try:
        r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
        r.raise_for_status()
        with open(zip_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except requests.exceptions.RequestException as e:
        logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink()
        return False

def log_failed_download(doc_id, submit_date, error_msg):
    """ã‚¹ãƒ†ãƒƒãƒ—3ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ã™ã‚‹"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    failed_record = pd.DataFrame([{
        "docID": doc_id,
        "submitDateTime": submit_date,
        "errorMessage": error_msg,
        "timestamp": timestamp
    }])
    failed_log_path = Config.FAILED_LOG_PATH
    failed_record.to_csv(failed_log_path, mode='a', header=not failed_log_path.exists(), index=False, encoding='utf_8_sig')

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)
        if not success:
            log_failed_download(doc_id, submit_date, "åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")

        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)

def retry_failed_downloads():
    """å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œã—ã€æˆåŠŸã—ãŸã‚‚ã®ã¯ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤ã™ã‚‹"""
    failed_log_path = Config.FAILED_LOG_PATH

    if not failed_log_path.exists():
        logging.info("å†è©¦è¡Œå¯¾è±¡ã®å¤±æ•—ãƒ­ã‚°ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    try:
        failed_df = pd.read_csv(failed_log_path, encoding='utf_8_sig')
    except Exception as e:
        logging.error(f"å¤±æ•—ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return

    if failed_df.empty:
        logging.info("å¤±æ•—ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    logging.info(f"ğŸ” {len(failed_df)} ä»¶ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
    successful_ids = []
    failed_again = [] # å†å¤±æ•—è¨˜éŒ²ç”¨

    for _, row in tqdm(failed_df.iterrows(), total=len(failed_df), desc="å†è©¦è¡Œä¸­"):
        doc_id = row['docID']
        submit_date = pd.to_datetime(row['submitDateTime'], errors='coerce')

        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)

        if success:
            successful_ids.append(doc_id)
        else:
            logging.warning(f"å†è©¦è¡Œå¤±æ•—: {doc_id}")
            failed_again.append((doc_id, submit_date))

        time.sleep(0.1)
        
    # æˆåŠŸã—ãŸIDã‚’ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤
    remaining_df = failed_df[~failed_df['docID'].isin(successful_ids)]
    remaining_df.to_csv(failed_log_path, index=False, encoding='utf_8_sig')

    # å†è¨˜éŒ²ï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã ã‘ï¼‰
    for doc_id, submit_date in failed_again:
        log_failed_download(doc_id, submit_date, "å†è©¦è¡Œå¤±æ•—")

    logging.info(f"âœ… å†è©¦è¡Œå®Œäº†ã€‚æˆåŠŸ: {len(successful_ids)} ä»¶ / æ®‹ã‚Š: {len(remaining_df) + len(failed_again)} ä»¶")
```

## Zipã®ä¸­èº«ç¢ºèª(2025-09-18)

zip_utils.py

```
from pathlib import Path
import zipfile
import logging

def inspect_zip_contents(zip_path: Path) -> list[str]:
    """
    æŒ‡å®šã•ã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§ï¼‰ã‚’è¿”ã™ã€‚
    """
    if not zip_path.exists():
        logging.warning(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {zip_path}")
        return []

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            logging.info(f"{zip_path.name} ã®å†…å®¹:")
            for f in file_list:
                logging.info(f"  - {f}")
            return file_list
    except zipfile.BadZipFile:
        logging.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {zip_path}")
        return []

def extract_xbrl_from_zip(zip_path: Path, extract_to: Path) -> list[Path]:
    """
    ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã€æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã€‚
    æŠ½å‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ä¸€è¦§ã‚’è¿”ã™ã€‚
    """
    extracted_files = []
    if not zip_path.exists():
        logging.warning(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {zip_path}")
        return []

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            for file_name in zip_ref.namelist():
                if file_name.lower().endswith(".xbrl"):
                    zip_ref.extract(file_name, path=extract_to)
                    extracted_files.append(extract_to / file_name)
        logging.info(f"{len(extracted_files)} ä»¶ã®XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        return extracted_files
    except zipfile.BadZipFile:
        logging.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {zip_path}")
        return []
```

test_zip_utils.py

```
import pytest
from pathlib import Path
from zip_utils import inspect_zip_contents, extract_xbrl_from_zip
import zipfile

@pytest.fixture
def sample_zip(tmp_path):
    """ä¸€æ™‚çš„ãªZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦è¿”ã™"""
    zip_path = tmp_path / "test_sample.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        zipf.writestr("XBRL/PublicDoc/sample.xbrl", "<xbrl>...</xbrl>")
        zipf.writestr("README.txt", "This is a test file.")
    return zip_path

def test_inspect_zip_contents_valid(sample_zip):
    """æ­£å¸¸ãªZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’ç¢ºèª"""
    contents = inspect_zip_contents(sample_zip)
    assert "XBRL/PublicDoc/sample.xbrl" in contents
    assert "README.txt" in contents
    assert len(contents) == 2

def test_inspect_zip_contents_missing():
    """å­˜åœ¨ã—ãªã„ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãŸå ´åˆ"""
    fake_path = Path("non_existent.zip")
    contents = inspect_zip_contents(fake_path)
    assert contents == []

def test_inspect_zip_contents_corrupt(tmp_path):
    """å£Šã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãŸå ´åˆ"""
    corrupt_zip = tmp_path / "corrupt.zip"
    corrupt_zip.write_text("ã“ã‚Œã¯ZIPã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    contents = inspect_zip_contents(corrupt_zip)
    assert contents == []

def test_extract_xbrl_from_zip_valid(sample_zip, tmp_path):
    """æ­£å¸¸ãªZIPã‹ã‚‰XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã§ãã‚‹ã‹"""
    extract_dir = tmp_path / "extracted"
    extracted_files = extract_xbrl_from_zip(sample_zip, extract_dir)

    assert len(extracted_files) == 1
    assert extracted_files[0].name == "sample.xbrl"
    assert extracted_files[0].exists()
    assert extracted_files[0].read_text().startswith("<xbrl>")

def test_extract_xbrl_from_zip_no_xbrl(tmp_path):
    """XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ãªã„ZIPã®å‡¦ç†"""
    zip_path = tmp_path / "no_xbrl.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        zipf.writestr("README.txt", "No XBRL here.")
    extract_dir = tmp_path / "extracted"
    extracted_files = extract_xbrl_from_zip(zip_path, extract_dir)

    assert extracted_files == []

def test_extract_xbrl_from_zip_corrupt(tmp_path):
    """å£Šã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""
    corrupt_zip = tmp_path / "corrupt.zip"
    corrupt_zip.write_text("Not a zip file")
    extract_dir = tmp_path / "extracted"
    extracted_files = extract_xbrl_from_zip(corrupt_zip, extract_dir)

    assert extracted_files == []
```

