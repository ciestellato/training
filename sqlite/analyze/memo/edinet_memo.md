# EDINET APIã§è‡ªåˆ†ã ã‘ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ãŸã„ï¼<!-- omit in toc -->

- [å‚è€ƒURL](#å‚è€ƒurl)
- [ä½œæ¥­ãƒ­ã‚°](#ä½œæ¥­ãƒ­ã‚°)
  - [EDINET APIæ¥ç¶šå®Œäº† (2025-09-17)](#edinet-apiæ¥ç¶šå®Œäº†-2025-09-17)
    - [1. APIã‚­ãƒ¼ã®ç™ºè¡Œ](#1-apiã‚­ãƒ¼ã®ç™ºè¡Œ)
    - [2. EDINET APIã§æƒ…å ±ã‚’å–å¾—ã™ã‚‹](#2-edinet-apiã§æƒ…å ±ã‚’å–å¾—ã™ã‚‹)
      - [ç’°å¢ƒæ§‹ç¯‰](#ç’°å¢ƒæ§‹ç¯‰)
      - [ã‚³ãƒ¼ãƒ‰](#ã‚³ãƒ¼ãƒ‰)
        - [edinet\_config.py](#edinet_configpy)
        - [edinet\_steps.py](#edinet_stepspy)
        - [edinet\_main.py](#edinet_mainpy)
      - [çµæœ](#çµæœ)
  - [é€²æ—ãƒ­ã‚°ã®CSVå‡ºåŠ›(2025-09-18)](#é€²æ—ãƒ­ã‚°ã®csvå‡ºåŠ›2025-09-18)
  - [ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãƒªã‚«ãƒãƒªãƒ¼(2025-09-18)](#ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãƒªã‚«ãƒãƒªãƒ¼2025-09-18)
  - [Zipã®ä¸­èº«ç¢ºèª(2025-09-18)](#zipã®ä¸­èº«ç¢ºèª2025-09-18)
  - [SQLiteã®ä¿å­˜](#sqliteã®ä¿å­˜)
  - [æ¯å›ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰Šé™¤ï¼†æ–°è¦ä½œæˆã‚’ä¿®æ­£](#æ¯å›ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰Šé™¤æ–°è¦ä½œæˆã‚’ä¿®æ­£)

## å‚è€ƒURL

1. [é‡‘èåº EDINET é–²è¦§ã‚µã‚¤ãƒˆ](https://disclosure2.edinet-fsa.go.jp/week0010.aspx)
2. [[EDINET] ä¸Šå ´ä¼æ¥­ã®æ¥­ç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’APIã§å–å¾—ã™ã‚‹](https://qiita.com/hifistar/items/0114c6f60ded96785178)
3. [ã€PythonÃ—EDINET APIã€‘CSVã§è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ï¼å·®åˆ†æ›´æ–°ã§æ§‹ç¯‰ã™ã‚‹é‡‘èåˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹](https://qiita.com/invest-aitech/items/7e13e89821bd754dfc25)


## ä½œæ¥­ãƒ­ã‚°

### EDINET APIæ¥ç¶šå®Œäº† (2025-09-17)

#### 1. APIã‚­ãƒ¼ã®ç™ºè¡Œ

å¿…è¦äº‹é …ã‚’å…¥åŠ›ã—ã¦ã€APIã‚­ãƒ¼ã‚’ç™ºè¡Œã™ã‚‹ã€‚

[EDINET API v2 ä»•æ§˜æ›¸](https://disclosure2dl.edinet-fsa.go.jp/guide/static/disclosure/download/ESE140206.pdf)

> 2-3-1 ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ä½œæˆã«ã¤ã„ã¦ 
> 
> EDINET API ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚
>  
> 2-3-1-1 ã‚µã‚¤ãƒ³ã‚¤ãƒ³ç”»é¢ã®è¡¨ç¤º 
> 
> ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹å ´åˆã¯ã€æ¬¡ã®URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ã‚µã‚¤ãƒ³ã‚¤ãƒ³ç”»é¢ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ 
> 
> https://api.edinet-fsa.go.jp/api/auth/index.aspx?mode=1

â€»ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£é™¤ã—ã¦ã„ãªã„ã¨APIã‚­ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„ã®ã§æ³¨æ„

#### 2. EDINET APIã§æƒ…å ±ã‚’å–å¾—ã™ã‚‹

å‚è€ƒURL3ã®Qiitaã®è¨˜äº‹ã¯Google Colaboratoryã¨Google Driveã ã£ãŸãŸã‚ã€ã¾ãšã¯ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ãã€‚

##### ç’°å¢ƒæ§‹ç¯‰

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸã‚‚ã®(ã™ã¹ã¦pipã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)

- requests
- pandas
- tqdm

`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’ä¿å­˜ã—ã¦ãŠã

##### ã‚³ãƒ¼ãƒ‰

Copilotã«ç›¸è«‡ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚

###### edinet_config.py

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm  # notebookã§ã¯ãªãé€šå¸¸ç‰ˆ
import warnings
from dotenv import load_dotenv  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãŠãï¼‰
env_path = Path(__file__).resolve().parents[2] / ".env"
load_dotenv(dotenv_path=env_path)

# --- è¨­å®šé …ç›® ---
class Config:
    """EDINETãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®è¨­å®šã‚¯ãƒ©ã‚¹"""
    # ã‚¢ã‚¯ã‚»ã‚¹è¨­å®š
    API_BASE_URL = "https://disclosure.edinet-fsa.go.jp/api/v2"
    REQUEST_TIMEOUT = 30
    DOWNLOAD_TIMEOUT = 60

    # åŸºæœ¬ãƒ‘ã‚¹ï¼ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    BASE_DIR = Path(os.getenv("EDINET_BASE_DIR", "C:/Users/0602JP/Documents/EDINET_DB/"))
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆ
    SAVE_FOLDER = BASE_DIR / "01_zip_files/"

    # APIã‚­ãƒ¼ã®å–å¾—ã¨ãƒã‚§ãƒƒã‚¯
    API_KEY = os.getenv("EDINET_API_KEY")
    if not API_KEY:
        raise ValueError("EDINET_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ã‚’æ‹…ä¿ã™ã‚‹ãŸã‚ã€ä½•æ—¥åˆ†é¡ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ã™ã‚‹ã‹
    RELIABILITY_DAYS = 7
    # åˆå›å®Ÿè¡Œæ™‚ã«ä½•å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹
    INITIAL_FETCH_YEARS = 1

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰
    # 120: æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸, 140: å››åŠæœŸå ±å‘Šæ›¸, 160: åŠæœŸå ±å‘Šæ›¸
    TARGET_DOC_TYPE_CODES = ['120', '140', '160']
```

###### edinet_steps.py

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False,
                timeout=Config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            res_json = response.json()
            if res_json.get('results'):
                new_docs.extend(res_json['results'])
        except requests.exceptions.RequestException as e:
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— - {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1)

    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        summary = pd.concat([summary, temp_df], ignore_index=True)

    summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
    summary.dropna(subset=['docID'], inplace=True)
    summary = summary.drop_duplicates(subset='docID', keep='last')
    summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)
    try:
        summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
        logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    return summary

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and "
        "secCode.notna() and secCode != 'None' and "
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}"
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("\nğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']

        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        if pd.isna(submit_date):
            target_folder = Config.SAVE_FOLDER / "unknown_date"
        else:
            year = submit_date.year
            quarter = (submit_date.month - 1) // 3 + 1
            target_folder = Config.SAVE_FOLDER / str(year) / f"Q{quarter}"

        target_folder.mkdir(parents=True, exist_ok=True)
        zip_path = target_folder / f"{doc_id}.zip"

        # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
        params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

        try:
            r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
            r.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        except requests.exceptions.RequestException as e:
            logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
            logging.debug(traceback.format_exc())
            if zip_path.exists():
                zip_path.unlink()
        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)
```

###### edinet_main.py

```
from edinet_steps import (
    step1_create_and_summarize,
    step2_check_download_status,
    step3_execute_download
)
import logging
import traceback
from edinet_config import Config
from pathlib import Path

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆã‚’æŒ‡å®š
log_path = Config.BASE_DIR / "edinet_log.txt"

# ãƒ­ã‚°è¨­å®šï¼šINFOä»¥ä¸Šã‚’è¡¨ç¤ºã€ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä¸¡æ–¹ã«å‡ºåŠ›
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def main() -> bool:
    """EDINETãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    try:
        logging.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        summary_data = step1_create_and_summarize()

        if summary_data.empty:
            logging.warning("ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            return False

        files_to_download = step2_check_download_status(summary_data)

        if files_to_download.empty:
            logging.info("æ–°è¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            step3_execute_download(files_to_download)

        logging.info("å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ğŸ‰")
        return True

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return False

if __name__ == '__main__':
    success = main()
    if not success:
        logging.warning("ä¸€éƒ¨ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
```

##### çµæœ

zipãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ãŸ

### é€²æ—ãƒ­ã‚°ã®CSVå‡ºåŠ›(2025-09-18)

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’CSVã§å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚

```
def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    log_records = []  # ãƒ­ã‚°è¨˜éŒ²ç”¨ãƒªã‚¹ãƒˆ

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        if pd.isna(submit_date):
            target_folder = Config.SAVE_FOLDER / "unknown_date"
        else:
            year = submit_date.year
            quarter = (submit_date.month - 1) // 3 + 1
            target_folder = Config.SAVE_FOLDER / str(year) / f"Q{quarter}"

        target_folder.mkdir(parents=True, exist_ok=True)
        zip_path = target_folder / f"{doc_id}.zip"

        # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
        params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

        try:
            r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
            r.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            status = "success"
            error_msg = ""
        except requests.exceptions.RequestException as e:
            logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
            logging.debug(traceback.format_exc())
            if zip_path.exists():
                zip_path.unlink()
            status = "failed"
            error_msg = str(e)

        log_records.append({
            "docID": doc_id,
            "submitDateTime": submit_date,
            "status": status,
            "errorMessage": error_msg,
            "savedPath": str(zip_path if status == "success" else ""),
            "timestamp": timestamp
        })

        time.sleep(0.1)
        
    # CSVã«ä¿å­˜
    log_df = pd.DataFrame(log_records)
    log_csv_path = Config.BASE_DIR / "download_log.csv"
    try:
        log_df.to_csv(log_csv_path, index=False, encoding='utf_8_sig')
        logging.info(f"ğŸ“„ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {log_csv_path}")
    except Exception as e:
        logging.error(f"ãƒ­ã‚°CSVã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)
```

### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãƒªã‚«ãƒãƒªãƒ¼(2025-09-18)

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’è¨˜éŒ²ã—ã¦ã€å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ä»•çµ„ã¿ã‚’ä½œã‚ŠãŸã„

- æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¨˜éŒ²å»ƒæ­¢
- å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¨˜éŒ²
- å¤±æ•—å±¥æ­´ã‹ã‚‰ã®å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã—ãŸã‚‰å¤±æ•—å±¥æ­´ã‹ã‚‰å‰Šé™¤
- mainãƒ¡ã‚½ãƒƒãƒ‰
  - å‰å›ã®å¤±æ•—ã®ãƒªãƒˆãƒ©ã‚¤
  - ä»Šå›ã®å‡¦ç†
  - ä»Šå›ã®å¤±æ•—ã®ãƒªãƒˆãƒ©ã‚¤

```
from edinet_steps import (
    step1_create_and_summarize,
    step2_check_download_status,
    step3_execute_download,
    retry_failed_downloads
)
import logging
import traceback
from edinet_config import Config
from pathlib import Path

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆã‚’æŒ‡å®š
log_path = Config.BASE_DIR / "edinet_log.txt"

# ãƒ­ã‚°è¨­å®šï¼šINFOä»¥ä¸Šã‚’è¡¨ç¤ºã€ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä¸¡æ–¹ã«å‡ºåŠ›
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def main() -> bool:
    """EDINETãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    try:
        logging.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        # ğŸ” Step 0: å‰å›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œ

        logging.info("ğŸ” Step 0: å‰å›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
        retry_failed_downloads()

        # ğŸ§¾ Step 1: ã‚µãƒãƒªãƒ¼ä½œæˆ

        summary_data = step1_create_and_summarize()
        if summary_data.empty:
            logging.warning("ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            return False

        # ğŸ“‹ Step 2: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æŠ½å‡º

        files_to_download = step2_check_download_status(summary_data)

        # ğŸ“¦ Step 3: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ

        if files_to_download.empty:
            logging.info("æ–°è¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            step3_execute_download(files_to_download)

        # ğŸ” Step 4: ä»Šå›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œ
        
        logging.info("ğŸ” Step 4: ä»Šå›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
        retry_failed_downloads()

        logging.info("å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ğŸ‰")
        return True

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return False

if __name__ == '__main__':
    success = main()
    if not success:
        logging.warning("ä¸€éƒ¨ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
```

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm  # notebookã§ã¯ãªãé€šå¸¸ç‰ˆ
import warnings
from dotenv import load_dotenv  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãŠãï¼‰
env_path = Path(__file__).resolve().parents[2] / ".env"
load_dotenv(dotenv_path=env_path)

# --- è¨­å®šé …ç›® ---
class Config:
    """EDINETãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®è¨­å®šã‚¯ãƒ©ã‚¹"""
    # ã‚¢ã‚¯ã‚»ã‚¹è¨­å®š
    API_BASE_URL = "https://disclosure.edinet-fsa.go.jp/api/v2"
    REQUEST_TIMEOUT = 30
    DOWNLOAD_TIMEOUT = 60

    # åŸºæœ¬ãƒ‘ã‚¹ï¼ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    BASE_DIR = Path(os.getenv("EDINET_BASE_DIR", "C:/Users/0602JP/Documents/EDINET_DB/"))
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆ
    SAVE_FOLDER = BASE_DIR / "01_zip_files/"
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ãƒ­ã‚°ã®ä¿å­˜å…ˆ
    FAILED_LOG_PATH = BASE_DIR / "failed_downloads.csv"

    # APIã‚­ãƒ¼ã®å–å¾—ã¨ãƒã‚§ãƒƒã‚¯
    API_KEY = os.getenv("EDINET_API_KEY")
    if not API_KEY:
        raise ValueError("EDINET_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ã‚’æ‹…ä¿ã™ã‚‹ãŸã‚ã€ä½•æ—¥åˆ†é¡ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ã™ã‚‹ã‹
    RELIABILITY_DAYS = 7
    # åˆå›å®Ÿè¡Œæ™‚ã«ä½•å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹
    INITIAL_FETCH_YEARS = 1

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰
    # 120: æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸, 140: å››åŠæœŸå ±å‘Šæ›¸, 160: åŠæœŸå ±å‘Šæ›¸
    TARGET_DOC_TYPE_CODES = ['120', '140', '160']
```

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False,
                timeout=Config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            res_json = response.json()
            if res_json.get('results'):
                new_docs.extend(res_json['results'])
        except requests.exceptions.RequestException as e:
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— - {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1)

    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        summary = pd.concat([summary, temp_df], ignore_index=True)

    summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
    summary.dropna(subset=['docID'], inplace=True)
    summary = summary.drop_duplicates(subset='docID', keep='last')
    summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)
    try:
        summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
        logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    return summary

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and "
        "secCode.notna() and secCode != 'None' and "
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}"
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("\nğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download

def download_single_file(doc_id: str, submit_date, save_folder: Path) -> bool:
    """1ä»¶ã®EDINETãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¿å­˜ã€‚æˆåŠŸãªã‚‰Trueã€å¤±æ•—ãªã‚‰False"""
    if pd.isna(submit_date):
        target_folder = save_folder / "unknown_date"
    else:
        year = submit_date.year
        quarter = (submit_date.month - 1) // 3 + 1
        target_folder = save_folder / str(year) / f"Q{quarter}"

    target_folder.mkdir(parents=True, exist_ok=True)
    zip_path = target_folder / f"{doc_id}.zip"

    # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
    params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

    try:
        r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
        r.raise_for_status()
        with open(zip_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except requests.exceptions.RequestException as e:
        logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink()
        return False

def log_failed_download(doc_id, submit_date, error_msg):
    """ã‚¹ãƒ†ãƒƒãƒ—3ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ã™ã‚‹"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    failed_record = pd.DataFrame([{
        "docID": doc_id,
        "submitDateTime": submit_date,
        "errorMessage": error_msg,
        "timestamp": timestamp
    }])
    failed_log_path = Config.FAILED_LOG_PATH
    failed_record.to_csv(failed_log_path, mode='a', header=not failed_log_path.exists(), index=False, encoding='utf_8_sig')

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)
        if not success:
            log_failed_download(doc_id, submit_date, "åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")

        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)

def retry_failed_downloads():
    """å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œã—ã€æˆåŠŸã—ãŸã‚‚ã®ã¯ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤ã™ã‚‹"""
    failed_log_path = Config.FAILED_LOG_PATH

    if not failed_log_path.exists():
        logging.info("å†è©¦è¡Œå¯¾è±¡ã®å¤±æ•—ãƒ­ã‚°ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    try:
        failed_df = pd.read_csv(failed_log_path, encoding='utf_8_sig')
    except Exception as e:
        logging.error(f"å¤±æ•—ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return

    if failed_df.empty:
        logging.info("å¤±æ•—ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    logging.info(f"ğŸ” {len(failed_df)} ä»¶ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
    successful_ids = []
    failed_again = [] # å†å¤±æ•—è¨˜éŒ²ç”¨

    for _, row in tqdm(failed_df.iterrows(), total=len(failed_df), desc="å†è©¦è¡Œä¸­"):
        doc_id = row['docID']
        submit_date = pd.to_datetime(row['submitDateTime'], errors='coerce')

        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)

        if success:
            successful_ids.append(doc_id)
        else:
            logging.warning(f"å†è©¦è¡Œå¤±æ•—: {doc_id}")
            failed_again.append((doc_id, submit_date))

        time.sleep(0.1)
        
    # æˆåŠŸã—ãŸIDã‚’ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤
    remaining_df = failed_df[~failed_df['docID'].isin(successful_ids)]
    remaining_df.to_csv(failed_log_path, index=False, encoding='utf_8_sig')

    # å†è¨˜éŒ²ï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã ã‘ï¼‰
    for doc_id, submit_date in failed_again:
        log_failed_download(doc_id, submit_date, "å†è©¦è¡Œå¤±æ•—")

    logging.info(f"âœ… å†è©¦è¡Œå®Œäº†ã€‚æˆåŠŸ: {len(successful_ids)} ä»¶ / æ®‹ã‚Š: {len(remaining_df) + len(failed_again)} ä»¶")
```

### Zipã®ä¸­èº«ç¢ºèª(2025-09-18)

zip_utils.py

```
from pathlib import Path
import zipfile
import logging

def inspect_zip_contents(zip_path: Path) -> list[str]:
    """
    æŒ‡å®šã•ã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§ï¼‰ã‚’è¿”ã™ã€‚
    """
    if not zip_path.exists():
        logging.warning(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {zip_path}")
        return []

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            logging.info(f"{zip_path.name} ã®å†…å®¹:")
            for f in file_list:
                logging.info(f"  - {f}")
            return file_list
    except zipfile.BadZipFile:
        logging.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {zip_path}")
        return []

def extract_xbrl_from_zip(zip_path: Path, extract_to: Path) -> list[Path]:
    """
    ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã€æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã€‚
    æŠ½å‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ä¸€è¦§ã‚’è¿”ã™ã€‚
    """
    extracted_files = []
    if not zip_path.exists():
        logging.warning(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {zip_path}")
        return []

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            for file_name in zip_ref.namelist():
                if file_name.lower().endswith(".xbrl"):
                    zip_ref.extract(file_name, path=extract_to)
                    extracted_files.append(extract_to / file_name)
        logging.info(f"{len(extracted_files)} ä»¶ã®XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        return extracted_files
    except zipfile.BadZipFile:
        logging.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {zip_path}")
        return []
```

test_zip_utils.py

```
import pytest
from pathlib import Path
from zip_utils import inspect_zip_contents, extract_xbrl_from_zip
import zipfile

@pytest.fixture
def sample_zip(tmp_path):
    """ä¸€æ™‚çš„ãªZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦è¿”ã™"""
    zip_path = tmp_path / "test_sample.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        zipf.writestr("XBRL/PublicDoc/sample.xbrl", "<xbrl>...</xbrl>")
        zipf.writestr("README.txt", "This is a test file.")
    return zip_path

def test_inspect_zip_contents_valid(sample_zip):
    """æ­£å¸¸ãªZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’ç¢ºèª"""
    contents = inspect_zip_contents(sample_zip)
    assert "XBRL/PublicDoc/sample.xbrl" in contents
    assert "README.txt" in contents
    assert len(contents) == 2

def test_inspect_zip_contents_missing():
    """å­˜åœ¨ã—ãªã„ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãŸå ´åˆ"""
    fake_path = Path("non_existent.zip")
    contents = inspect_zip_contents(fake_path)
    assert contents == []

def test_inspect_zip_contents_corrupt(tmp_path):
    """å£Šã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãŸå ´åˆ"""
    corrupt_zip = tmp_path / "corrupt.zip"
    corrupt_zip.write_text("ã“ã‚Œã¯ZIPã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    contents = inspect_zip_contents(corrupt_zip)
    assert contents == []

def test_extract_xbrl_from_zip_valid(sample_zip, tmp_path):
    """æ­£å¸¸ãªZIPã‹ã‚‰XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã§ãã‚‹ã‹"""
    extract_dir = tmp_path / "extracted"
    extracted_files = extract_xbrl_from_zip(sample_zip, extract_dir)

    assert len(extracted_files) == 1
    assert extracted_files[0].name == "sample.xbrl"
    assert extracted_files[0].exists()
    assert extracted_files[0].read_text().startswith("<xbrl>")

def test_extract_xbrl_from_zip_no_xbrl(tmp_path):
    """XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ãªã„ZIPã®å‡¦ç†"""
    zip_path = tmp_path / "no_xbrl.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        zipf.writestr("README.txt", "No XBRL here.")
    extract_dir = tmp_path / "extracted"
    extracted_files = extract_xbrl_from_zip(zip_path, extract_dir)

    assert extracted_files == []

def test_extract_xbrl_from_zip_corrupt(tmp_path):
    """å£Šã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""
    corrupt_zip = tmp_path / "corrupt.zip"
    corrupt_zip.write_text("Not a zip file")
    extract_dir = tmp_path / "extracted"
    extracted_files = extract_xbrl_from_zip(corrupt_zip, extract_dir)

    assert extracted_files == []
```

check_zip.py

```
from pathlib import Path
import zipfile

def preview_zip_contents(zip_path: Path, max_files: int = 50):
    """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’è¡¨ç¤ºã™ã‚‹"""
    if not zip_path.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {zip_path}")
        return

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«: {zip_path.name}")
            print(f"ğŸ“ å«ã¾ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_list)}")
            print("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
            for i, f in enumerate(file_list):
                if i >= max_files:
                    print("...ï¼ˆçœç•¥ï¼‰")
                    break
                print(f"  - {f}")
    except zipfile.BadZipFile:
        print(f"âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {zip_path}")

def extract_csv_from_zip(zip_path: str, extract_to: str = "./extracted_csv"):
    """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹"""
    zip_path = Path(zip_path)
    extract_to = Path(extract_to)
    extract_to.mkdir(parents=True, exist_ok=True)

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]
            zip_ref.extractall(path=extract_to, members=csv_files)
            return [str(extract_to / f) for f in csv_files]
    except zipfile.BadZipFile:
        print(f"âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {zip_path}")
        return []
```

test_check_zip.py

```
import pytest
from pathlib import Path
from edinet_config import Config
from check_zip import extract_csv_from_zip

def get_latest_zip_file() -> Path:
    zip_files = sorted(Config.SAVE_FOLDER.rglob("*.zip"))
    if not zip_files:
        raise FileNotFoundError(f"No ZIP files found in {Config.SAVE_FOLDER}")
    return zip_files[-1]

def test_extract_csv_from_zip():
    zip_path = get_latest_zip_file()
    extract_to = Path("./test_output")
    extract_to.mkdir(exist_ok=True)

    extracted_files = extract_csv_from_zip(str(zip_path), extract_to=str(extract_to))

    assert len(extracted_files) > 0, "CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
    for file in extracted_files:
        assert file.endswith(".csv")
        print(f"âœ… æŠ½å‡ºã•ã‚ŒãŸCSV: {file}")
```

### SQLiteã®ä¿å­˜

edinet_main.py

```
from edinet_steps import (
    step1_create_and_summarize,
    step2_check_download_status,
    step3_execute_download,
    retry_failed_downloads,
    step_store_summary_to_db,
    step_extract_and_index_csv 
)
import logging
import traceback
from edinet_config import Config
from pathlib import Path

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆã‚’æŒ‡å®š
log_path = Config.BASE_DIR / "edinet_log.txt"

# ãƒ­ã‚°è¨­å®šï¼šINFOä»¥ä¸Šã‚’è¡¨ç¤ºã€ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä¸¡æ–¹ã«å‡ºåŠ›
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def main() -> bool:
    """EDINETãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    try:
        logging.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        # ğŸ” Step 0: å‰å›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œ

        logging.info("ğŸ” Step 0: å‰å›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
        retry_failed_downloads()

        # ğŸ§¾ Step 1: ã‚µãƒãƒªãƒ¼ä½œæˆ

        summary_data = step1_create_and_summarize()
        if summary_data.empty:
            logging.warning("ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            return False

        # ğŸ“‹ Step 2: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æŠ½å‡º

        files_to_download = step2_check_download_status(summary_data)

        # ğŸ“¦ Step 3: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ

        if files_to_download.empty:
            logging.info("æ–°è¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            step3_execute_download(files_to_download)

        # ğŸ” Step 4: ä»Šå›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œ

        logging.info("ğŸ” Step 4: ä»Šå›ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
        retry_failed_downloads()

        # ğŸ“Š Step 5: ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteã«ä¿ç®¡
        step_store_summary_to_db(summary_data)

        # ğŸ“„ Step 6: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ZIPã‹ã‚‰CSVã‚’æŠ½å‡ºã—ã€ãƒ‘ã‚¹ã‚’SQLiteã«è¨˜éŒ²
        step_extract_and_index_csv(Config.SAVE_FOLDER)

        logging.info("å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ğŸ‰")
        return True

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return False

if __name__ == '__main__':
    success = main()
    if not success:
        logging.warning("ä¸€éƒ¨ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
```

edinet_config.py

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm  # notebookã§ã¯ãªãé€šå¸¸ç‰ˆ
import warnings
from dotenv import load_dotenv  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãŠãï¼‰
env_path = Path(__file__).resolve().parents[2] / ".env"
load_dotenv(dotenv_path=env_path)

# --- è¨­å®šé …ç›® ---
class Config:
    """EDINETãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®è¨­å®šã‚¯ãƒ©ã‚¹"""
    # ã‚¢ã‚¯ã‚»ã‚¹è¨­å®š
    API_BASE_URL = "https://disclosure.edinet-fsa.go.jp/api/v2"
    REQUEST_TIMEOUT = 30
    DOWNLOAD_TIMEOUT = 60

    # åŸºæœ¬ãƒ‘ã‚¹ï¼ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    BASE_DIR = Path(os.getenv("EDINET_BASE_DIR", "C:/Users/0602JP/Documents/EDINET_DB/"))
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆ
    SAVE_FOLDER = BASE_DIR / "01_zip_files/"
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ãƒ­ã‚°ã®ä¿å­˜å…ˆ
    FAILED_LOG_PATH = BASE_DIR / "failed_downloads.csv"

    # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    DB_PATH = BASE_DIR / "edinet_data.db"
    
    # CSVä¸€æ™‚æŠ½å‡ºãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ (SQLiteæ ¼ç´å¾Œã€å‰Šé™¤ã‚’æ¤œè¨)
    EXTRACTED_CSV_TEMP_FOLDER = BASE_DIR / "02_extracted_csv_temp/"

    # APIã‚­ãƒ¼ã®å–å¾—ã¨ãƒã‚§ãƒƒã‚¯
    API_KEY = os.getenv("EDINET_API_KEY")
    if not API_KEY:
        raise ValueError("EDINET_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ã‚’æ‹…ä¿ã™ã‚‹ãŸã‚ã€ä½•æ—¥åˆ†é¡ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ã™ã‚‹ã‹
    RELIABILITY_DAYS = 7
    # åˆå›å®Ÿè¡Œæ™‚ã«ä½•å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹
    INITIAL_FETCH_YEARS = 1

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰
    # 120: æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸, 140: å››åŠæœŸå ±å‘Šæ›¸, 160: åŠæœŸå ±å‘Šæ›¸
    TARGET_DOC_TYPE_CODES = ['120', '140', '160']
```

edinet_steps.py

```
import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback
import sqlite3 # SQLiteã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# zip_utilsã‹ã‚‰CSVæŠ½å‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from zip_utils import extract_csv_from_zip 

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()

    # Config.INITIAL_FETCH_YEARS ã‹ã‚‰ Config.RELIABILITY_DAYS ã‚’è€ƒæ…®ã—ã¦ start_day ã‚’è¨­å®š
    # æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ INITIAL_FETCH_YEARS åˆ†é¡ã‚‹
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                # ä¿¡é ¼æ€§ç¢ºä¿ã®ãŸã‚ã€æœ€æ–°æ—¥ä»˜ã‹ã‚‰RELIABILITY_DAYSåˆ†é¡ã£ã¦å†å–å¾—é–‹å§‹
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        # EDINET APIä»•æ§˜æ›¸ [32, 34] ã«åŸºã¥ããƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False, # æœ¬ç•ªç’°å¢ƒã§ã¯Trueã‚’æ¤œè¨
                timeout=Config.REQUEST_TIMEOUT
            )
            # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ4xx/5xxã®å ´åˆã¯ã“ã“ã§RequestExceptionã‚’ç™ºç”Ÿã•ã›ã‚‹
            # ãŸã ã—ã€EDINET APIã¯ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚HTTP 200ã‚’è¿”ã™å ´åˆãŒã‚ã‚‹ãŸã‚ã€JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æã‚‚å¿…è¦ [1]
            response.raise_for_status()
            res_json = response.json()
            logging.debug(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ (æ—¥: {day.strftime('%Y-%m-%d')}): {res_json}") # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’å‡ºåŠ›

            status = None
            message = None

            # EDINET APIã®ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’è€ƒæ…®ã—ã¦statusã¨messageã‚’å–å¾— [2, 3]
            if isinstance(res_json, dict):
                if 'metadata' in res_json and isinstance(res_json['metadata'], dict):
                    metadata = res_json['metadata']
                    status = metadata.get('status')
                    message = metadata.get('message')
                elif 'StatusCode' in res_json: # 401 Access denied ã®å ´åˆãªã© [3]
                    status = res_json.get('StatusCode')
                    message = res_json.get('message')
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã«å¿œã˜ãŸå‡¦ç† [1]
            if status == '404' or status == 404: 
                logging.info(f"æƒ…å ±ãªã—: {day.strftime('%Y-%m-%d')} ã®æ›¸é¡ä¸€è¦§ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            elif status and (str(status) != '200'): # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ200ä»¥å¤–ã®å ´åˆ
                log_msg = f"APIã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} - Status: {status}, Message: {message if message else 'è©³ç´°ä¸æ˜'}"
                logging.warning(log_msg)
            elif res_json.get('results'): # æ­£å¸¸ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§'results'ãŒã‚ã‚‹å ´åˆ [6]
                new_docs.extend(res_json['results'])
            elif status == '200' and not res_json.get('results'): # **ğŸ‘ˆ ã“ã“ã«æ–°ã—ã„ elif ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—ã¾ã™**
                # APIã¯æ­£å¸¸å¿œç­” (status: 200, message: OK) ã ãŒã€resultsãŒç©ºã®å ´åˆ
                logging.info(f"æƒ…å ±ãªã—: {day.strftime('%Y-%m-%d')} ã®æå‡ºæ›¸é¡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                logging.debug(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ (æ—¥: {day.strftime('%Y-%m-%d')}): {res_json}") # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’å‡ºåŠ›
            else: # ãã®ä»–ã®äºˆæœŸã›ã¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ (ã“ã‚Œã«å…¥ã‚‹ã“ã¨ã¯ç¨€ã«ãªã‚‹ã¯ãš)
                logging.warning(f"äºˆæœŸã›ã¬APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—: {day.strftime('%Y-%m-%d')}. ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {res_json}")

        except requests.exceptions.RequestException as e:
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ4xx/5xxã ã£ãŸå ´åˆ
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¾ãŸã¯HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼) - {e}")
            logging.debug(traceback.format_exc())
        except ValueError as e: # response.json()ãŒJSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆ
            logging.error(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒJSONã¨ã—ã¦è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}, ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response.text[:500]}...")
            logging.debug(traceback.format_exc())
        except Exception as e: # ãã®ä»–ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼
            logging.error(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1) # APIã¸ã®è² è·è»½æ¸›ã®ãŸã‚
        
    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        # æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€é‡è¤‡ã‚’æ’é™¤
        # docIDã¯æå‡ºæ›¸é¡ã”ã¨ã«ä»˜ä¸ã•ã‚Œã‚‹ä¸€æ„ã®ç•ªå· [40, 102]
        summary = pd.concat([summary, temp_df], ignore_index=True) 
        summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
        summary.dropna(subset=['docID'], inplace=True)
        summary = summary.drop_duplicates(subset='docID', keep='last')
        summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)

        try:
            summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
            logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        except Exception as e:
            logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())
    else:
        logging.info("æ–°è¦ã«å–å¾—ã•ã‚ŒãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    return summary

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and " # CSVæœ‰ç„¡ãƒ•ãƒ©ã‚°ãŒ'1' [44]
        "secCode.notna() and secCode != 'None' and " # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}" # å¯¾è±¡æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰ [107]
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("ğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download

def download_single_file(doc_id: str, submit_date, save_folder: Path) -> bool:
    """1ä»¶ã®EDINETãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¿å­˜ã€‚æˆåŠŸãªã‚‰Trueã€å¤±æ•—ãªã‚‰False"""
    if pd.isna(submit_date):
        target_folder = save_folder / "unknown_date"
    else:
        year = submit_date.year
        quarter = (submit_date.month - 1) // 3 + 1
        target_folder = save_folder / str(year) / f"Q{quarter}"

    target_folder.mkdir(parents=True, exist_ok=True)
    zip_path = target_folder / f"{doc_id}.zip"

    # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ [89]
    url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
    # å¿…è¦æ›¸é¡ã‚¿ã‚¤ãƒ— '5' ã¯CSVå½¢å¼ã®ZIPãƒ•ã‚¡ã‚¤ãƒ« [90, 93]
    params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

    try:
        r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
        r.raise_for_status()

        # Content-Typeã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª [92, 99]
        content_type = r.headers.get('Content-Type', '')
        if 'application/octet-stream' not in content_type: # ZIPå½¢å¼ã®å ´åˆ
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒJSONå½¢å¼ã§è¿”ã£ã¦ãã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…® [92]
            if 'application/json' in content_type:
                error_json = r.json()
                status = error_json.get('metadata', {}).get('status', 'N/A')
                message = error_json.get('metadata', {}).get('message', 'N/A')
                logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}. APIãŒã‚¨ãƒ©ãƒ¼JSONã‚’è¿”å´ã€‚Status: {status}, Message: {message}")
            else:
                logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}. äºˆæœŸã›ã¬Content-Type: {content_type}")
            if zip_path.exists():
                zip_path.unlink() # ä¸å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            return False

        with open(zip_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        return True

    except requests.exceptions.RequestException as e:
        logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink() # å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        return False
    except Exception as e:
        logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {doc_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink()
        return False
    
def log_failed_download(doc_id, submit_date, error_msg):
    """ã‚¹ãƒ†ãƒƒãƒ—3ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ã™ã‚‹"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    failed_record = pd.DataFrame([{
        "docID": doc_id,
        "submitDateTime": submit_date,
        "errorMessage": error_msg,
        "timestamp": timestamp
    }])
    failed_log_path = Config.FAILED_LOG_PATH
    failed_record.to_csv(failed_log_path, mode='a', header=not failed_log_path.exists(), index=False, encoding='utf_8_sig')

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)
        if not success:
            log_failed_download(doc_id, submit_date, "åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")

        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)

def retry_failed_downloads():
    """å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œã—ã€æˆåŠŸã—ãŸã‚‚ã®ã¯ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤ã™ã‚‹"""
    failed_log_path = Config.FAILED_LOG_PATH

    if not failed_log_path.exists():
        logging.info("å†è©¦è¡Œå¯¾è±¡ã®å¤±æ•—ãƒ­ã‚°ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    try:
        failed_df = pd.read_csv(failed_log_path, encoding='utf_8_sig')
    except Exception as e:
        logging.error(f"å¤±æ•—ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return

    if failed_df.empty:
        logging.info("å¤±æ•—ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    logging.info(f"ğŸ” {len(failed_df)} ä»¶ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
    successful_ids = []
    failed_again = [] # å†å¤±æ•—è¨˜éŒ²ç”¨

    for _, row in tqdm(failed_df.iterrows(), total=len(failed_df), desc="å†è©¦è¡Œä¸­"):
        doc_id = row['docID']
        submit_date = pd.to_datetime(row['submitDateTime'], errors='coerce')

        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)

        if success:
            successful_ids.append(doc_id)
        else:
            logging.warning(f"å†è©¦è¡Œå¤±æ•—: {doc_id}")
            failed_again.append((doc_id, submit_date))

        time.sleep(0.1)
        
    # æˆåŠŸã—ãŸIDã‚’ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤
    remaining_df = failed_df[~failed_df['docID'].isin(successful_ids)]
    remaining_df.to_csv(failed_log_path, index=False, encoding='utf_8_sig')

    # å†è¨˜éŒ²ï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã ã‘ï¼‰
    for doc_id, submit_date in failed_again:
        log_failed_download(doc_id, submit_date, "å†è©¦è¡Œå¤±æ•—")

    logging.info(f"âœ… å†è©¦è¡Œå®Œäº†ã€‚æˆåŠŸ: {len(successful_ids)} ä»¶ / æ®‹ã‚Š: {len(remaining_df) + len(failed_again)} ä»¶")

def step_store_summary_to_db(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘£: å–å¾—ã—ãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«å: edinet_document_summaries
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘£ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ ---")
    if summary_df.empty:
        logging.warning("ä¿ç®¡ã™ã‚‹ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    conn = None
    try:
        conn = sqlite3.connect(Config.DB_PATH)
        logging.info(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{Config.DB_PATH.name}' ã«æ¥ç¶šã—ã¾ã—ãŸã€‚")

        # æå‡ºæ›¸é¡ä¸€è¦§ã®JSONæ§‹é€  [38-44] ã‚’è€ƒæ…®ã—ã€DataFrameã‚’SQLiteã«æ›¸ãè¾¼ã¿
        # docIDã¯ä¸€æ„ãªãŸã‚ãƒ—ãƒ©ã‚¤ãƒãƒªã‚­ãƒ¼ã¨ã—ã¦è¨­å®š
        summary_df.to_sql(
            "edinet_document_summaries",
            conn,
            if_exists='replace', # æ¯å›å…¨ä»¶ç½®ãæ›ãˆ (é‹ç”¨ã«åˆã‚ã›ã¦ 'append' ã‚„ 'upsert' ã‚’æ¤œè¨)
            index=False,
            dtype={
                'seqNumber': 'INTEGER',
                'docID': 'TEXT PRIMARY KEY', # docIDã‚’ä¸»ã‚­ãƒ¼ã«è¨­å®š
                'edinetCode': 'TEXT',
                'secCode': 'TEXT',
                'JCN': 'TEXT',
                'filerName': 'TEXT',
                'fundCode': 'TEXT',
                'ordinanceCode': 'TEXT',
                'formCode': 'TEXT',
                'docTypeCode': 'TEXT',
                'periodStart': 'TEXT',
                'periodEnd': 'TEXT',
                'submitDateTime': 'TIMESTAMP',
                'docDescription': 'TEXT',
                'issuerEdinetCode': 'TEXT',
                'subjectEdinetCode': 'TEXT',
                'subsidiaryEdinetCode': 'TEXT',
                'currentReportReason': 'TEXT',
                'parentDocID': 'TEXT',
                'opeDateTime': 'TIMESTAMP',
                'withdrawalStatus': 'TEXT',
                'docInfoEditStatus': 'TEXT',
                'disclosureStatus': 'TEXT',
                'xbrlFlag': 'TEXT',
                'pdfFlag': 'TEXT',
                'attachDocFlag': 'TEXT',
                'englishDocFlag': 'TEXT',
                'csvFlag': 'TEXT',
                'legalStatus': 'TEXT'
            }
        )
        logging.info(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ 'edinet_document_summaries' ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿ç®¡ã—ã¾ã—ãŸï¼ˆ{len(summary_df)} ä»¶ï¼‰ã€‚")

    except sqlite3.Error as e:
        logging.error(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    finally:
        if conn:
            conn.close()
            logging.info("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
    logging.info("-" * 40)

def step_extract_and_index_csv(zip_base_folder: Path):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¤: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã€ãã®ãƒ‘ã‚¹ã‚’SQLiteã«è¨˜éŒ²ã™ã‚‹ã€‚
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸€æ™‚æŠ½å‡ºãƒ•ã‚©ãƒ«ãƒ€ã«ä¿ç®¡ã•ã‚Œã‚‹ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«å: edinet_extracted_csv_details
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¤ CSVæŠ½å‡ºã¨æŠ½å‡ºãƒ‘ã‚¹ã®SQLiteè¨˜éŒ² ---")

    # ä¸€æ™‚æŠ½å‡ºãƒ•ã‚©ãƒ«ãƒ€ã‚’æº–å‚™
    extract_temp_folder = Config.EXTRACTED_CSV_TEMP_FOLDER
    extract_temp_folder.mkdir(parents=True, exist_ok=True)

    all_zip_files = list(zip_base_folder.rglob('*.zip'))
    if not all_zip_files:
        logging.info("å‡¦ç†å¯¾è±¡ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    conn = None
    try:
        conn = sqlite3.connect(Config.DB_PATH)
        logging.info(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{Config.DB_PATH.name}' ã«æ¥ç¶šã—ã¾ã—ãŸã€‚")

        # CSVæŠ½å‡ºæƒ…å ±ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        conn.execute("""
            CREATE TABLE IF NOT EXISTS edinet_extracted_csv_details (
                docID TEXT NOT NULL,
                csv_filename TEXT NOT NULL,
                extracted_path TEXT PRIMARY KEY,
                extraction_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(docID) REFERENCES edinet_document_summaries(docID)
            )
        """)
        conn.commit()

        processed_zip_count = 0 # å‡¦ç†ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°
        total_extracted_csv_count = 0 # æŠ½å‡ºã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç·æ•°
        inserted_path_count = 0 # DBã«è¨˜éŒ²ã—ãŸCSVãƒ‘ã‚¹ã®ç·æ•°

        for zip_file_path in tqdm(all_zip_files, desc="CSVæŠ½å‡ºã¨DBè¨˜éŒ²"):
            doc_id = zip_file_path.stem # ZIPãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰docIDã‚’å–å¾—
            try:
                # æŠ½å‡ºå…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ docID ã”ã¨ã«åˆ†ã‘ã‚‹ã“ã¨ã§ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡çªã‚’é˜²ã
                current_extract_folder = extract_temp_folder / doc_id
                
                extracted_csv_paths = extract_csv_from_zip(zip_file_path, current_extract_folder)

                if extracted_csv_paths:
                    processed_zip_count += 1
                    total_extracted_csv_count += len(extracted_csv_paths)

                    for csv_path in extracted_csv_paths:
                        # æŠ½å‡ºãƒ‘ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
                        conn.execute(
                            "INSERT OR REPLACE INTO edinet_extracted_csv_details (docID, csv_filename, extracted_path) VALUES (?, ?, ?)",
                            (doc_id, csv_path.name, str(csv_path))
                        )
                        inserted_path_count += 1
                    conn.commit() # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚³ãƒŸãƒƒãƒˆ
                # else: extract_csv_from_zip å†…ã§ debug/warning ãŒå‡ºåŠ›ã•ã‚Œã‚‹

            except Exception as e:
                logging.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ« '{zip_file_path.name}' ã®CSVæŠ½å‡ºãƒ»DBè¨˜éŒ²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                logging.debug(traceback.format_exc())
        
        logging.info(
            f"âœ… CSVæŠ½å‡ºã¨SQLiteè¨˜éŒ²å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
            f"è¨ˆ {processed_zip_count} ä»¶ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {total_extracted_csv_count} ä»¶ã®CSVã‚’æŠ½å‡ºã—ã€"
            f"{inserted_path_count} ä»¶ã®CSVãƒ‘ã‚¹ã‚’DBã«è¨˜éŒ²ã—ã¾ã—ãŸã€‚"
        )

    except sqlite3.Error as e:
        logging.error(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    except Exception as e:
        logging.error(f"CSVæŠ½å‡ºãƒ»DBè¨˜éŒ²å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    finally:
        if conn:
            conn.close()
            logging.info("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
    
    # å¿…è¦ã«å¿œã˜ã¦ä¸€æ™‚æŠ½å‡ºãƒ•ã‚©ãƒ«ãƒ€ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è¿½åŠ 
    # ä¾‹: shutil.rmtree(extract_temp_folder)
    # ãŸã ã—ã€å¾Œã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’åˆ†æã™ã‚‹ãŸã‚ã«æ®‹ã—ã¦ãŠãã“ã¨ã‚‚å¤šã„ã®ã§ã€ã“ã“ã§ã¯è‡ªå‹•å‰Šé™¤ã¯ã—ãªã„ã€‚

    logging.info("-" * 40)
```

### æ¯å›ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰Šé™¤ï¼†æ–°è¦ä½œæˆã‚’ä¿®æ­£

edinet_steps.py

- é–¢æ•°åã‚’step~ã‚’step4~ step5~ã«å¤‰æ›´
- å¤‰æ›´å‰
  - æ¯å›ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã‚’å‰Šé™¤ã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§å†ä½œæˆ
- å¤‰æ›´å¾Œ
  - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¯å›ç½®ãæ›ãˆã‚‹ã®ã§ã¯ãªãã€æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯è¿½åŠ ã—ã€æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯æ›´æ–°ã™ã‚‹ (upsert) ã‚ˆã†ã«å¤‰æ›´

```
def step4_store_summary_to_db(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘£: å–å¾—ã—ãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«å: edinet_document_summaries
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘£ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ ---")
    if summary_df.empty:
        logging.warning("ä¿ç®¡ã™ã‚‹ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    conn = None
    try:
        conn = sqlite3.connect(Config.DB_PATH)
        logging.info(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{Config.DB_PATH.name}' ã«æ¥ç¶šã—ã¾ã—ãŸã€‚")

        # 1. ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆã™ã‚‹
        #    docIDã¯ä¸€æ„ã®è­˜åˆ¥å­ã¨ã—ã¦PRIMARY KEYã«è¨­å®šã™ã‚‹
        columns_with_types = {
            'seqNumber': 'INTEGER',
            'docID': 'TEXT PRIMARY KEY', # docIDã‚’ä¸»ã‚­ãƒ¼ã¨ã—ã¦å®šç¾©
            'edinetCode': 'TEXT',
            'secCode': 'TEXT',
            'JCN': 'TEXT',
            'filerName': 'TEXT',
            'fundCode': 'TEXT',
            'ordinanceCode': 'TEXT',
            'formCode': 'TEXT',
            'docTypeCode': 'TEXT',
            'periodStart': 'TEXT',
            'periodEnd': 'TEXT',
            'submitDateTime': 'TIMESTAMP',
            'docDescription': 'TEXT',
            'issuerEdinetCode': 'TEXT',
            'subjectEdinetCode': 'TEXT',
            'subsidiaryEdinetCode': 'TEXT',
            'currentReportReason': 'TEXT',
            'parentDocID': 'TEXT',
            'opeDateTime': 'TIMESTAMP',
            'withdrawalStatus': 'TEXT',
            'docInfoEditStatus': 'TEXT',
            'disclosureStatus': 'TEXT',
            'xbrlFlag': 'TEXT',
            'pdfFlag': 'TEXT',
            'attachDocFlag': 'TEXT',
            'englishDocFlag': 'TEXT',
            'csvFlag': 'TEXT',
            'legalStatus': 'TEXT'
        }
        
        create_table_sql_parts = [f"{col_name} {col_type}" for col_name, col_type in columns_with_types.items()]
        create_table_sql = f"CREATE TABLE IF NOT EXISTS edinet_document_summaries ({', '.join(create_table_sql_parts)})"
        conn.execute(create_table_sql)
        conn.commit()

        # 2. ç¾åœ¨ã®ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€
        #    ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã¯docIDã‚’PRIMARY KEYã«ã™ã‚‹å¿…è¦ã¯ãªã„ãŒã€å‹ã¯ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã«åˆã‚ã›ã‚‹
        temp_dtype_map = {k: v.replace(' PRIMARY KEY', '') for k, v in columns_with_types.items()}
        summary_df.to_sql(
            "temp_edinet_document_summaries", # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«å
            conn,
            if_exists='replace', # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã¯æ¯å›ç½®ãæ›ãˆã‚‹
            index=False,
            dtype=temp_dtype_map
        )
        
        # 3. INSERT OR REPLACE ã‚’ä½¿ã£ã¦ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ãƒ‡ãƒ¼ã‚¿ã‚’ç§»å‹•ã™ã‚‹
        #    ã“ã‚Œã«ã‚ˆã‚Šã€docIDãŒæ—¢å­˜ã®å ´åˆã¯ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒæ›´æ–°ã•ã‚Œã€æ–°ã—ã„å ´åˆã¯æŒ¿å…¥ã•ã‚Œã‚‹
        columns = ', '.join(summary_df.columns)
        conn.execute(f"""
            INSERT OR REPLACE INTO edinet_document_summaries ({columns})
            SELECT {columns} FROM temp_edinet_document_summaries
        """)
        conn.commit()

        # 4. ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã™ã‚‹
        conn.execute("DROP TABLE temp_edinet_document_summaries")
        conn.commit()

        logging.info(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ 'edinet_document_summaries' ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿ç®¡ã—ã¾ã—ãŸï¼ˆ{len(summary_df)} ä»¶ï¼‰ã€‚"
                     f"æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯æ›´æ–°ã•ã‚Œã€æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚")

    except sqlite3.Error as e:
        logging.error(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    finally:
        if conn:
            conn.close()
            logging.info("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
    logging.info("-" * 40)
```
