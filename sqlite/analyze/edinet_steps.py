import os
import time
import requests
import pandas as pd
from datetime import date, timedelta
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False,
                timeout=Config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            res_json = response.json()
            if res_json.get('results'):
                new_docs.extend(res_json['results'])
        except requests.exceptions.RequestException as e:
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— - {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1)

    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        summary = pd.concat([summary, temp_df], ignore_index=True)

    summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
    summary.dropna(subset=['docID'], inplace=True)
    summary = summary.drop_duplicates(subset='docID', keep='last')
    summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)
    try:
        summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
        logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    return summary

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and "
        "secCode.notna() and secCode != 'None' and "
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}"
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("\nğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']

        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        if pd.isna(submit_date):
            target_folder = Config.SAVE_FOLDER / "unknown_date"
        else:
            year = submit_date.year
            quarter = (submit_date.month - 1) // 3 + 1
            target_folder = Config.SAVE_FOLDER / str(year) / f"Q{quarter}"

        target_folder.mkdir(parents=True, exist_ok=True)
        zip_path = target_folder / f"{doc_id}.zip"

        # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
        params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

        try:
            r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
            r.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        except requests.exceptions.RequestException as e:
            logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
            logging.debug(traceback.format_exc())
            if zip_path.exists():
                zip_path.unlink()
        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)