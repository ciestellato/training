import os
import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback
import sqlite3 # SQLiteã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# zip_utilsã‹ã‚‰CSVæŠ½å‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from zip_utils import extract_csv_from_zip 

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()

    # Config.INITIAL_FETCH_YEARS ã‹ã‚‰ Config.RELIABILITY_DAYS ã‚’è€ƒæ…®ã—ã¦ start_day ã‚’è¨­å®š
    # æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ INITIAL_FETCH_YEARS åˆ†é¡ã‚‹
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                # ä¿¡é ¼æ€§ç¢ºä¿ã®ãŸã‚ã€æœ€æ–°æ—¥ä»˜ã‹ã‚‰RELIABILITY_DAYSåˆ†é¡ã£ã¦å†å–å¾—é–‹å§‹
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        # EDINET APIä»•æ§˜æ›¸ [32, 34] ã«åŸºã¥ããƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False, # æœ¬ç•ªç’°å¢ƒã§ã¯Trueã‚’æ¤œè¨
                timeout=Config.REQUEST_TIMEOUT
            )
            # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ4xx/5xxã®å ´åˆã¯ã“ã“ã§RequestExceptionã‚’ç™ºç”Ÿã•ã›ã‚‹
            # ãŸã ã—ã€EDINET APIã¯ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚HTTP 200ã‚’è¿”ã™å ´åˆãŒã‚ã‚‹ãŸã‚ã€JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æã‚‚å¿…è¦ [1]
            response.raise_for_status()
            res_json = response.json()
            logging.debug(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ (æ—¥: {day.strftime('%Y-%m-%d')}): {res_json}") # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’å‡ºåŠ›

            status = None
            message = None

            # EDINET APIã®ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’è€ƒæ…®ã—ã¦statusã¨messageã‚’å–å¾— [2, 3]
            if isinstance(res_json, dict):
                if 'metadata' in res_json and isinstance(res_json['metadata'], dict):
                    metadata = res_json['metadata']
                    status = metadata.get('status')
                    message = metadata.get('message')
                elif 'StatusCode' in res_json: # 401 Access denied ã®å ´åˆãªã© [3]
                    status = res_json.get('StatusCode')
                    message = res_json.get('message')
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã«å¿œã˜ãŸå‡¦ç† [1]
            if status == '404' or status == 404: 
                logging.info(f"æƒ…å ±ãªã—: {day.strftime('%Y-%m-%d')} ã®æ›¸é¡ä¸€è¦§ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            elif status and (str(status) != '200'): # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ200ä»¥å¤–ã®å ´åˆ
                log_msg = f"APIã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} - Status: {status}, Message: {message if message else 'è©³ç´°ä¸æ˜'}"
                logging.warning(log_msg)
            elif res_json.get('results'): # æ­£å¸¸ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§'results'ãŒã‚ã‚‹å ´åˆ [6]
                new_docs.extend(res_json['results'])
            elif status == '200' and not res_json.get('results'): # **ğŸ‘ˆ ã“ã“ã«æ–°ã—ã„ elif ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—ã¾ã™**
                # APIã¯æ­£å¸¸å¿œç­” (status: 200, message: OK) ã ãŒã€resultsãŒç©ºã®å ´åˆ
                logging.info(f"æƒ…å ±ãªã—: {day.strftime('%Y-%m-%d')} ã®æå‡ºæ›¸é¡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                logging.debug(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ (æ—¥: {day.strftime('%Y-%m-%d')}): {res_json}") # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’å‡ºåŠ›
            else: # ãã®ä»–ã®äºˆæœŸã›ã¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ (ã“ã‚Œã«å…¥ã‚‹ã“ã¨ã¯ç¨€ã«ãªã‚‹ã¯ãš)
                logging.warning(f"äºˆæœŸã›ã¬APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—: {day.strftime('%Y-%m-%d')}. ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {res_json}")

        except requests.exceptions.RequestException as e:
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ4xx/5xxã ã£ãŸå ´åˆ
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¾ãŸã¯HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼) - {e}")
            logging.debug(traceback.format_exc())
        except ValueError as e: # response.json()ãŒJSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆ
            logging.error(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒJSONã¨ã—ã¦è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}, ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response.text[:500]}...")
            logging.debug(traceback.format_exc())
        except Exception as e: # ãã®ä»–ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼
            logging.error(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1) # APIã¸ã®è² è·è»½æ¸›ã®ãŸã‚
        
    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        # æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€é‡è¤‡ã‚’æ’é™¤
        # docIDã¯æå‡ºæ›¸é¡ã”ã¨ã«ä»˜ä¸ã•ã‚Œã‚‹ä¸€æ„ã®ç•ªå· [40, 102]
        summary = pd.concat([summary, temp_df], ignore_index=True) 
        summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
        summary.dropna(subset=['docID'], inplace=True)
        summary = summary.drop_duplicates(subset='docID', keep='last')
        summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)

        try:
            summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
            logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        except Exception as e:
            logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())
    else:
        logging.info("æ–°è¦ã«å–å¾—ã•ã‚ŒãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    return summary

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and " # CSVæœ‰ç„¡ãƒ•ãƒ©ã‚°ãŒ'1' [44]
        "secCode.notna() and secCode != 'None' and " # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}" # å¯¾è±¡æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰ [107]
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("ğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download

def download_single_file(doc_id: str, submit_date, save_folder: Path) -> bool:
    """1ä»¶ã®EDINETãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¿å­˜ã€‚æˆåŠŸãªã‚‰Trueã€å¤±æ•—ãªã‚‰False"""
    if pd.isna(submit_date):
        target_folder = save_folder / "unknown_date"
    else:
        year = submit_date.year
        quarter = (submit_date.month - 1) // 3 + 1
        target_folder = save_folder / str(year) / f"Q{quarter}"

    target_folder.mkdir(parents=True, exist_ok=True)
    zip_path = target_folder / f"{doc_id}.zip"

    # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ [89]
    url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
    # å¿…è¦æ›¸é¡ã‚¿ã‚¤ãƒ— '5' ã¯CSVå½¢å¼ã®ZIPãƒ•ã‚¡ã‚¤ãƒ« [90, 93]
    params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

    try:
        r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
        r.raise_for_status()

        # Content-Typeã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª [92, 99]
        content_type = r.headers.get('Content-Type', '')
        if 'application/octet-stream' not in content_type: # ZIPå½¢å¼ã®å ´åˆ
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒJSONå½¢å¼ã§è¿”ã£ã¦ãã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…® [92]
            if 'application/json' in content_type:
                error_json = r.json()
                status = error_json.get('metadata', {}).get('status', 'N/A')
                message = error_json.get('metadata', {}).get('message', 'N/A')
                logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}. APIãŒã‚¨ãƒ©ãƒ¼JSONã‚’è¿”å´ã€‚Status: {status}, Message: {message}")
            else:
                logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}. äºˆæœŸã›ã¬Content-Type: {content_type}")
            if zip_path.exists():
                zip_path.unlink() # ä¸å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            return False

        with open(zip_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        return True

    except requests.exceptions.RequestException as e:
        logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink() # å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        return False
    except Exception as e:
        logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {doc_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink()
        return False
    
def log_failed_download(doc_id, submit_date, error_msg):
    """ã‚¹ãƒ†ãƒƒãƒ—3ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ã™ã‚‹"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    failed_record = pd.DataFrame([{
        "docID": doc_id,
        "submitDateTime": submit_date,
        "errorMessage": error_msg,
        "timestamp": timestamp
    }])
    failed_log_path = Config.FAILED_LOG_PATH
    failed_record.to_csv(failed_log_path, mode='a', header=not failed_log_path.exists(), index=False, encoding='utf_8_sig')

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)
        if not success:
            log_failed_download(doc_id, submit_date, "åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")

        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)

def retry_failed_downloads():
    """å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œã—ã€æˆåŠŸã—ãŸã‚‚ã®ã¯ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤ã™ã‚‹"""
    failed_log_path = Config.FAILED_LOG_PATH

    if not failed_log_path.exists():
        logging.info("å†è©¦è¡Œå¯¾è±¡ã®å¤±æ•—ãƒ­ã‚°ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    try:
        failed_df = pd.read_csv(failed_log_path, encoding='utf_8_sig')
    except Exception as e:
        logging.error(f"å¤±æ•—ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return

    if failed_df.empty:
        logging.info("å¤±æ•—ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    logging.info(f"ğŸ” {len(failed_df)} ä»¶ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
    successful_ids = []
    failed_again = [] # å†å¤±æ•—è¨˜éŒ²ç”¨

    for _, row in tqdm(failed_df.iterrows(), total=len(failed_df), desc="å†è©¦è¡Œä¸­"):
        doc_id = row['docID']
        submit_date = pd.to_datetime(row['submitDateTime'], errors='coerce')

        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)

        if success:
            successful_ids.append(doc_id)
        else:
            logging.warning(f"å†è©¦è¡Œå¤±æ•—: {doc_id}")
            failed_again.append((doc_id, submit_date))

        time.sleep(0.1)
        
    # æˆåŠŸã—ãŸIDã‚’ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤
    remaining_df = failed_df[~failed_df['docID'].isin(successful_ids)]
    remaining_df.to_csv(failed_log_path, index=False, encoding='utf_8_sig')

    # å†è¨˜éŒ²ï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã ã‘ï¼‰
    for doc_id, submit_date in failed_again:
        log_failed_download(doc_id, submit_date, "å†è©¦è¡Œå¤±æ•—")

    logging.info(f"âœ… å†è©¦è¡Œå®Œäº†ã€‚æˆåŠŸ: {len(successful_ids)} ä»¶ / æ®‹ã‚Š: {len(remaining_df) + len(failed_again)} ä»¶")

def step5_store_summary_to_db(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘£: å–å¾—ã—ãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«å: edinet_document_summaries
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘£ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ ---")
    if summary_df.empty:
        logging.warning("ä¿ç®¡ã™ã‚‹ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    conn = None
    try:
        conn = sqlite3.connect(Config.DB_PATH)
        logging.info(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{Config.DB_PATH.name}' ã«æ¥ç¶šã—ã¾ã—ãŸã€‚")

        # 1. ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆã™ã‚‹
        #    docIDã¯ä¸€æ„ã®è­˜åˆ¥å­ã¨ã—ã¦PRIMARY KEYã«è¨­å®šã™ã‚‹
        columns_with_types = {
            'seqNumber': 'INTEGER',
            'docID': 'TEXT PRIMARY KEY', # docIDã‚’ä¸»ã‚­ãƒ¼ã¨ã—ã¦å®šç¾©
            'edinetCode': 'TEXT',
            'secCode': 'TEXT',
            'JCN': 'TEXT',
            'filerName': 'TEXT',
            'fundCode': 'TEXT',
            'ordinanceCode': 'TEXT',
            'formCode': 'TEXT',
            'docTypeCode': 'TEXT',
            'periodStart': 'TEXT',
            'periodEnd': 'TEXT',
            'submitDateTime': 'TIMESTAMP',
            'docDescription': 'TEXT',
            'issuerEdinetCode': 'TEXT',
            'subjectEdinetCode': 'TEXT',
            'subsidiaryEdinetCode': 'TEXT',
            'currentReportReason': 'TEXT',
            'parentDocID': 'TEXT',
            'opeDateTime': 'TIMESTAMP',
            'withdrawalStatus': 'TEXT',
            'docInfoEditStatus': 'TEXT',
            'disclosureStatus': 'TEXT',
            'xbrlFlag': 'TEXT',
            'pdfFlag': 'TEXT',
            'attachDocFlag': 'TEXT',
            'englishDocFlag': 'TEXT',
            'csvFlag': 'TEXT',
            'legalStatus': 'TEXT'
        }
        
        create_table_sql_parts = [f"{col_name} {col_type}" for col_name, col_type in columns_with_types.items()]
        create_table_sql = f"CREATE TABLE IF NOT EXISTS edinet_document_summaries ({', '.join(create_table_sql_parts)})"
        conn.execute(create_table_sql)
        conn.commit()

        # 2. ç¾åœ¨ã®ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€
        #    ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã¯docIDã‚’PRIMARY KEYã«ã™ã‚‹å¿…è¦ã¯ãªã„ãŒã€å‹ã¯ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã«åˆã‚ã›ã‚‹
        temp_dtype_map = {k: v.replace(' PRIMARY KEY', '') for k, v in columns_with_types.items()}
        summary_df.to_sql(
            "temp_edinet_document_summaries", # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«å
            conn,
            if_exists='replace', # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã¯æ¯å›ç½®ãæ›ãˆã‚‹
            index=False,
            dtype=temp_dtype_map
        )
        
        # 3. INSERT OR REPLACE ã‚’ä½¿ã£ã¦ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ãƒ‡ãƒ¼ã‚¿ã‚’ç§»å‹•ã™ã‚‹
        #    ã“ã‚Œã«ã‚ˆã‚Šã€docIDãŒæ—¢å­˜ã®å ´åˆã¯ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒæ›´æ–°ã•ã‚Œã€æ–°ã—ã„å ´åˆã¯æŒ¿å…¥ã•ã‚Œã‚‹
        columns = ', '.join(summary_df.columns)
        conn.execute(f"""
            INSERT OR REPLACE INTO edinet_document_summaries ({columns})
            SELECT {columns} FROM temp_edinet_document_summaries
        """)
        conn.commit()

        # 4. ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã™ã‚‹
        conn.execute("DROP TABLE temp_edinet_document_summaries")
        conn.commit()

        logging.info(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ 'edinet_document_summaries' ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿ç®¡ã—ã¾ã—ãŸï¼ˆ{len(summary_df)} ä»¶ï¼‰ã€‚"
                     f"æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯æ›´æ–°ã•ã‚Œã€æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚")

    except sqlite3.Error as e:
        logging.error(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    except Exception as e:
        logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    finally:
        if conn:
            conn.close()
            logging.info("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
    logging.info("-" * 40)
    
def step6_extract_and_index_csv(zip_base_folder: Path):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¤: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã€ãã®ãƒ‘ã‚¹ã‚’SQLiteã«è¨˜éŒ²ã™ã‚‹ã€‚
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸€æ™‚æŠ½å‡ºãƒ•ã‚©ãƒ«ãƒ€ã«ä¿ç®¡ã•ã‚Œã‚‹ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«å: edinet_extracted_csv_details
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¤ CSVæŠ½å‡ºã¨æŠ½å‡ºãƒ‘ã‚¹ã®SQLiteè¨˜éŒ² ---")

    # ä¸€æ™‚æŠ½å‡ºãƒ•ã‚©ãƒ«ãƒ€ã‚’æº–å‚™
    extract_temp_folder = Config.EXTRACTED_CSV_TEMP_FOLDER
    extract_temp_folder.mkdir(parents=True, exist_ok=True)

    all_zip_files = list(zip_base_folder.rglob('*.zip'))
    if not all_zip_files:
        logging.info("å‡¦ç†å¯¾è±¡ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    conn = None
    try:
        conn = sqlite3.connect(Config.DB_PATH)
        logging.info(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{Config.DB_PATH.name}' ã«æ¥ç¶šã—ã¾ã—ãŸã€‚")

        # CSVæŠ½å‡ºæƒ…å ±ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        conn.execute("""
            CREATE TABLE IF NOT EXISTS edinet_extracted_csv_details (
                docID TEXT NOT NULL,
                csv_filename TEXT NOT NULL,
                extracted_path TEXT PRIMARY KEY,
                extraction_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(docID) REFERENCES edinet_document_summaries(docID)
            )
        """)
        conn.commit()

        processed_zip_count = 0 # å‡¦ç†ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°
        total_extracted_csv_count = 0 # æŠ½å‡ºã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç·æ•°
        inserted_path_count = 0 # DBã«è¨˜éŒ²ã—ãŸCSVãƒ‘ã‚¹ã®ç·æ•°

        for zip_file_path in tqdm(all_zip_files, desc="CSVæŠ½å‡ºã¨DBè¨˜éŒ²"):
            doc_id = zip_file_path.stem # ZIPãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰docIDã‚’å–å¾—
            try:
                # æŠ½å‡ºå…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ docID ã”ã¨ã«åˆ†ã‘ã‚‹ã“ã¨ã§ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡çªã‚’é˜²ã
                current_extract_folder = extract_temp_folder / doc_id
                
                extracted_csv_paths = extract_csv_from_zip(zip_file_path, current_extract_folder)

                if extracted_csv_paths:
                    processed_zip_count += 1
                    total_extracted_csv_count += len(extracted_csv_paths)

                    for csv_path in extracted_csv_paths:
                        # æŠ½å‡ºãƒ‘ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
                        conn.execute(
                            "INSERT OR REPLACE INTO edinet_extracted_csv_details (docID, csv_filename, extracted_path) VALUES (?, ?, ?)",
                            (doc_id, csv_path.name, str(csv_path))
                        )
                        inserted_path_count += 1
                    conn.commit() # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚³ãƒŸãƒƒãƒˆ
                # else: extract_csv_from_zip å†…ã§ debug/warning ãŒå‡ºåŠ›ã•ã‚Œã‚‹

            except Exception as e:
                logging.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ« '{zip_file_path.name}' ã®CSVæŠ½å‡ºãƒ»DBè¨˜éŒ²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                logging.debug(traceback.format_exc())
        
        logging.info(
            f"âœ… CSVæŠ½å‡ºã¨SQLiteè¨˜éŒ²å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
            f"è¨ˆ {processed_zip_count} ä»¶ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {total_extracted_csv_count} ä»¶ã®CSVã‚’æŠ½å‡ºã—ã€"
            f"{inserted_path_count} ä»¶ã®CSVãƒ‘ã‚¹ã‚’DBã«è¨˜éŒ²ã—ã¾ã—ãŸã€‚"
        )

    except sqlite3.Error as e:
        logging.error(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    except Exception as e:
        logging.error(f"CSVæŠ½å‡ºãƒ»DBè¨˜éŒ²å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    finally:
        if conn:
            conn.close()
            logging.info("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
    
    # å¿…è¦ã«å¿œã˜ã¦ä¸€æ™‚æŠ½å‡ºãƒ•ã‚©ãƒ«ãƒ€ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è¿½åŠ 
    # ä¾‹: shutil.rmtree(extract_temp_folder)
    # ãŸã ã—ã€å¾Œã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’åˆ†æã™ã‚‹ãŸã‚ã«æ®‹ã—ã¦ãŠãã“ã¨ã‚‚å¤šã„ã®ã§ã€ã“ã“ã§ã¯è‡ªå‹•å‰Šé™¤ã¯ã—ãªã„ã€‚

    logging.info("-" * 40)

def step7_parse_and_store_csv_data_to_db():
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¦: æŠ½å‡ºã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã€è²¡å‹™æ•°å€¤ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«å: edinet_financial_data (ä¾‹)
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¦ CSVè§£æã¨è²¡å‹™æ•°å€¤ã®SQLiteä¿ç®¡ ---")

    conn = None
    try:
        conn = sqlite3.connect(Config.DB_PATH)
        logging.info(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{Config.DB_PATH.name}' ã«æ¥ç¶šã—ã¾ã—ãŸã€‚")

        # è²¡å‹™ãƒ‡ãƒ¼ã‚¿æ ¼ç´ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        # docID, secCode, fiscalYear, term, accountName ã®çµ„ã¿åˆã‚ã›ã‚’ä¸»ã‚­ãƒ¼ã«ã™ã‚‹ã“ã¨ã‚’æ¤œè¨
        conn.execute("""
            CREATE TABLE IF NOT EXISTS edinet_financial_data (
                docID TEXT NOT NULL,
                secCode TEXT,
                fiscalYear INTEGER,
                term TEXT,
                accountName TEXT NOT NULL,
                amount REAL,
                unit TEXT,
                currency TEXT,
                PRIMARY KEY (docID, accountName, fiscalYear, term),
                FOREIGN KEY(docID) REFERENCES edinet_document_summaries(docID)
            )
        """)
        conn.commit()

        # edinet_extracted_csv_details ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€å‡¦ç†å¯¾è±¡ã®CSVãƒ‘ã‚¹ã‚’å–å¾—
        # ã¾ã è§£æã•ã‚Œã¦ã„ãªã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„
        # (ä¾‹: æ–°ã—ã„ãƒ•ãƒ©ã‚°ã‚’ edinet_extracted_csv_details ã«è¿½åŠ ã—ã€å‡¦ç†æ¸ˆã¿ã‚’ãƒãƒ¼ã‚¯ã™ã‚‹ãªã©)
        csv_paths_df = pd.read_sql_query(
            "SELECT docID, extracted_path FROM edinet_extracted_csv_details", conn
        )

        if csv_paths_df.empty:
            logging.info("è§£æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        all_financial_data = []
        processed_csv_count = 0

        for _, row in tqdm(csv_paths_df.iterrows(), total=len(csv_paths_df), desc="CSVãƒ•ã‚¡ã‚¤ãƒ«è§£æ"):
            doc_id = row['docID']
            csv_path = Path(row['extracted_path'])

            if not csv_path.exists():
                logging.warning(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {csv_path}ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            try:
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€è²¡å‹™æ•°å€¤ã‚’è§£æã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«è¨˜è¿°
                # ä¾‹: df_csv = pd.read_csv(csv_path)
                #    # EDINET CSVã®æ§‹é€ ã«åˆã‚ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºãƒ»æ•´å½¢
                #    # ä¾‹ãˆã°ã€ç‰¹å®šã®åˆ—ã‚’å‹˜å®šç§‘ç›®ã€é‡‘é¡ã¨ã—ã¦æŠ½å‡º
                #    # temp_data = df_csv[['å‹˜å®šç§‘ç›®å', 'é‡‘é¡', 'å˜ä½', 'æœŸé–“']]
                #    # temp_data['docID'] = doc_id
                #    # all_financial_data.append(temp_data)

                # ã“ã“ã§ã¯å…·ä½“çš„ãªCSVãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒä¸æ˜ãªãŸã‚ã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¾‹ç¤º
                # å®Ÿéš›ã®EDINET CSVã®æ§‹é€ ã«åˆã‚ã›ã¦è§£æå‡¦ç†ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
                dummy_data = {
                    'docID': doc_id,
                    'secCode': 'xxxx', # docIDã‹ã‚‰å–å¾—ã™ã‚‹ã€ã¾ãŸã¯edinet_document_summariesã‹ã‚‰çµåˆ
                    'fiscalYear': 2023,
                    'term': 'Annual',
                    'accountName': 'å£²ä¸Šé«˜',
                    'amount': 100000000,
                    'unit': 'å††',
                    'currency': 'JPY'
                }
                all_financial_data.append(dummy_data)
                processed_csv_count += 1

            except Exception as e:
                logging.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ« '{csv_path.name}' ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                logging.debug(traceback.format_exc())

        if all_financial_data:
            financial_df = pd.DataFrame(all_financial_data)
            
            # DataFrameã‚’SQLiteãƒ†ãƒ¼ãƒ–ãƒ«ã«æ ¼ç´ (INSERT OR REPLACEã§é‡è¤‡ã‚’é¿ã‘ã€æ›´æ–°)
            # if_exists='append' ã¨ã—ã€INSERT OR REPLACE ã‚’ç›´æ¥SQLã§å®Ÿè¡Œã™ã‚‹æ–¹ãŒæŸ”è»Ÿæ€§ãŒã‚ã‚Šã¾ã™
            # pandas.to_sql ã§ã¯ if_exists='replace' ã¾ãŸã¯ 'append' ã—ã‹é¸ã¹ãªã„ãŸã‚ã€
            # ç‹¬è‡ªã®upsertãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã™ã‚‹ã‹ã€ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«çµŒç”±ã§INSERT OR REPLACEã‚’è¡Œã†
            # å‰å›ã® `step_store_summary_to_db` ã¨åŒæ§˜ã®æ‰‹æ³•ãŒåˆ©ç”¨å¯èƒ½ã§ã™
            
            # --- Upsertãƒ­ã‚¸ãƒƒã‚¯ã®ä¾‹ï¼ˆå‰å›ã®summary_dfã¨åŒæ§˜ï¼‰ ---
            # 1. ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€
            financial_df.to_sql(
                "temp_edinet_financial_data",
                conn,
                if_exists='replace',
                index=False,
                dtype={
                    'docID': 'TEXT',
                    'secCode': 'TEXT',
                    'fiscalYear': 'INTEGER',
                    'term': 'TEXT',
                    'accountName': 'TEXT',
                    'amount': 'REAL',
                    'unit': 'TEXT',
                    'currency': 'TEXT'
                }
            )
            # 2. INSERT OR REPLACE ã§ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ç§»å‹•
            columns = ', '.join(financial_df.columns)
            conn.execute(f"""
                INSERT OR REPLACE INTO edinet_financial_data ({columns})
                SELECT {columns} FROM temp_edinet_financial_data
            """)
            conn.commit()
            # 3. ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
            conn.execute("DROP TABLE temp_edinet_financial_data")
            conn.commit()
            # --- Upsertãƒ­ã‚¸ãƒƒã‚¯ã®ä¾‹ çµ‚äº† ---

            logging.info(f"âœ… {processed_csv_count} ä»¶ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã€"
                         f"{len(financial_df)} ä»¶ã®è²¡å‹™æ•°å€¤ã‚’ 'edinet_financial_data' ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿ç®¡ã—ã¾ã—ãŸã€‚")
        else:
            logging.info("è§£æãƒ»ä¿ç®¡ã•ã‚ŒãŸè²¡å‹™æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    except sqlite3.Error as e:
        logging.error(f"SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    except Exception as e:
        logging.error(f"CSVè§£æã¨è²¡å‹™æ•°å€¤ã®SQLiteä¿ç®¡ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
    finally:
        if conn:
            conn.close()
            logging.info("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
    logging.info("-" * 40)
