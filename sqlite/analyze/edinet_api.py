import time
import pandas as pd
import logging
import traceback
import requests
from tqdm import tqdm
from pathlib import Path
from datetime import date, timedelta, datetime

from edinet_config import Config

"""è²¬ä»»ç¯„å›²ï¼šEDINET APIã¨ã®ç›´æ¥çš„ãªé€šä¿¡å‡¦ç†ã€‚"""

def update_summary_file(base_dir: Path, api_key: str) -> pd.DataFrame:
    """EDINETã‹ã‚‰æ—¥æ¬¡ã®æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    summary_path = base_dir / "EDINET_Summary_v3.csv"
    logging.info(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« '{summary_path.name}' ã®çŠ¶æ…‹ã‚’ç¢ºèªãƒ»æ›´æ–°ã—ã¾ã™...")

    today = date.today()
    summary = pd.DataFrame()

    # Config.INITIAL_FETCH_YEARS ã‹ã‚‰ Config.RELIABILITY_DAYS ã‚’è€ƒæ…®ã—ã¦ start_day ã‚’è¨­å®š
    # æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ INITIAL_FETCH_YEARS åˆ†é¡ã‚‹
    start_day = today - timedelta(days=365 * Config.INITIAL_FETCH_YEARS)

    if summary_path.exists():
        try:
            dtype_map = {'secCode': str, 'docTypeCode': str, 'xbrlFlag': str, 'csvFlag': str}
            summary = pd.read_csv(summary_path, encoding='utf_8_sig', dtype=dtype_map)
            summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')

            if not summary.empty and 'submitDateTime' in summary.columns and not summary['submitDateTime'].isnull().all():
                latest_date_in_file = summary['submitDateTime'].max().date()
                # ä¿¡é ¼æ€§ç¢ºä¿ã®ãŸã‚ã€æœ€æ–°æ—¥ä»˜ã‹ã‚‰RELIABILITY_DAYSåˆ†é¡ã£ã¦å†å–å¾—é–‹å§‹
                start_day = latest_date_in_file - timedelta(days=Config.RELIABILITY_DAYS)
        except Exception as e:
            logging.warning(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())

    end_day = today
    day_term = [start_day + timedelta(days=i) for i in range((end_day - start_day).days + 1)]

    new_docs = []
    for day in tqdm(day_term, desc="APIã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"):
        # EDINET APIä»•æ§˜æ›¸ [32, 34] ã«åŸºã¥ããƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {'date': day.strftime('%Y-%m-%d'), 'type': 2, 'Subscription-Key': api_key}
        try:
            response = requests.get(
                Config.API_BASE_URL + "/documents.json",
                params=params,
                verify=False, # æœ¬ç•ªç’°å¢ƒã§ã¯Trueã‚’æ¤œè¨
                timeout=Config.REQUEST_TIMEOUT
            )
            # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ4xx/5xxã®å ´åˆã¯ã“ã“ã§RequestExceptionã‚’ç™ºç”Ÿã•ã›ã‚‹
            # ãŸã ã—ã€EDINET APIã¯ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚HTTP 200ã‚’è¿”ã™å ´åˆãŒã‚ã‚‹ãŸã‚ã€JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æã‚‚å¿…è¦ [1]
            response.raise_for_status()
            res_json = response.json()
            logging.debug(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ (æ—¥: {day.strftime('%Y-%m-%d')}): {res_json}") # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’å‡ºåŠ›

            status = None
            message = None

            # EDINET APIã®ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’è€ƒæ…®ã—ã¦statusã¨messageã‚’å–å¾— [2, 3]
            if isinstance(res_json, dict):
                if 'metadata' in res_json and isinstance(res_json['metadata'], dict):
                    metadata = res_json['metadata']
                    status = metadata.get('status')
                    message = metadata.get('message')
                elif 'StatusCode' in res_json: # 401 Access denied ã®å ´åˆãªã© [3]
                    status = res_json.get('StatusCode')
                    message = res_json.get('message')
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã«å¿œã˜ãŸå‡¦ç† [1]
            if status == '404' or status == 404: 
                logging.info(f"æƒ…å ±ãªã—: {day.strftime('%Y-%m-%d')} ã®æ›¸é¡ä¸€è¦§ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            elif status and (str(status) != '200'): # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ200ä»¥å¤–ã®å ´åˆ
                log_msg = f"APIã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} - Status: {status}, Message: {message if message else 'è©³ç´°ä¸æ˜'}"
                logging.warning(log_msg)
            elif res_json.get('results'): # æ­£å¸¸ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§'results'ãŒã‚ã‚‹å ´åˆ [6]
                new_docs.extend(res_json['results'])
            elif status == '200' and not res_json.get('results'): # **ğŸ‘ˆ ã“ã“ã«æ–°ã—ã„ elif ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã—ã¾ã™**
                # APIã¯æ­£å¸¸å¿œç­” (status: 200, message: OK) ã ãŒã€resultsãŒç©ºã®å ´åˆ
                logging.info(f"æƒ…å ±ãªã—: {day.strftime('%Y-%m-%d')} ã®æå‡ºæ›¸é¡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                logging.debug(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ (æ—¥: {day.strftime('%Y-%m-%d')}): {res_json}") # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’å‡ºåŠ›
            else: # ãã®ä»–ã®äºˆæœŸã›ã¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ (ã“ã‚Œã«å…¥ã‚‹ã“ã¨ã¯ç¨€ã«ãªã‚‹ã¯ãš)
                logging.warning(f"äºˆæœŸã›ã¬APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—: {day.strftime('%Y-%m-%d')}. ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {res_json}")

        except requests.exceptions.RequestException as e:
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ4xx/5xxã ã£ãŸå ´åˆ
            logging.warning(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¾ãŸã¯HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼) - {e}")
            logging.debug(traceback.format_exc())
        except ValueError as e: # response.json()ãŒJSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆ
            logging.error(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒJSONã¨ã—ã¦è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}, ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response.text[:500]}...")
            logging.debug(traceback.format_exc())
        except Exception as e: # ãã®ä»–ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼
            logging.error(f"ã‚¨ãƒ©ãƒ¼: {day.strftime('%Y-%m-%d')} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())
        time.sleep(0.1) # APIã¸ã®è² è·è»½æ¸›ã®ãŸã‚
        
    if new_docs:
        temp_df = pd.DataFrame(new_docs)
        # æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€é‡è¤‡ã‚’æ’é™¤
        # docIDã¯æå‡ºæ›¸é¡ã”ã¨ã«ä»˜ä¸ã•ã‚Œã‚‹ä¸€æ„ã®ç•ªå· [40, 102]
        summary = pd.concat([summary, temp_df], ignore_index=True) 
        summary['submitDateTime'] = pd.to_datetime(summary['submitDateTime'], errors='coerce')
        summary.dropna(subset=['docID'], inplace=True)
        summary = summary.drop_duplicates(subset='docID', keep='last')
        summary = summary.sort_values(by='submitDateTime', ascending=True).reset_index(drop=True)

        try:
            summary.to_csv(summary_path, index=False, encoding='utf_8_sig')
            logging.info("âœ… ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        except Exception as e:
            logging.error(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logging.debug(traceback.format_exc())
    else:
        logging.info("æ–°è¦ã«å–å¾—ã•ã‚ŒãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    return summary

def download_single_file(doc_id: str, submit_date, save_folder: Path) -> bool:
    """1ä»¶ã®EDINETãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¿å­˜ã€‚æˆåŠŸãªã‚‰Trueã€å¤±æ•—ãªã‚‰False"""
    if pd.isna(submit_date):
        target_folder = save_folder / "unknown_date"
    else:
        year = submit_date.year
        quarter = (submit_date.month - 1) // 3 + 1
        target_folder = save_folder / str(year) / f"Q{quarter}"

    target_folder.mkdir(parents=True, exist_ok=True)
    zip_path = target_folder / f"{doc_id}.zip"

    # EDINET API ã®ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ [89]
    url_zip = f"{Config.API_BASE_URL}/documents/{doc_id}"
    # å¿…è¦æ›¸é¡ã‚¿ã‚¤ãƒ— '5' ã¯CSVå½¢å¼ã®ZIPãƒ•ã‚¡ã‚¤ãƒ« [90, 93]
    params_zip = {"type": 5, 'Subscription-Key': Config.API_KEY}

    try:
        r = requests.get(url_zip, params=params_zip, stream=True, verify=False, timeout=Config.DOWNLOAD_TIMEOUT)
        r.raise_for_status()

        # Content-Typeã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª [92, 99]
        content_type = r.headers.get('Content-Type', '')
        if 'application/octet-stream' not in content_type: # ZIPå½¢å¼ã®å ´åˆ
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒJSONå½¢å¼ã§è¿”ã£ã¦ãã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…® [92]
            if 'application/json' in content_type:
                error_json = r.json()
                status = error_json.get('metadata', {}).get('status', 'N/A')
                message = error_json.get('metadata', {}).get('message', 'N/A')
                logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}. APIãŒã‚¨ãƒ©ãƒ¼JSONã‚’è¿”å´ã€‚Status: {status}, Message: {message}")
            else:
                logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}. äºˆæœŸã›ã¬Content-Type: {content_type}")
            if zip_path.exists():
                zip_path.unlink() # ä¸å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            return False

        with open(zip_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        return True

    except requests.exceptions.RequestException as e:
        logging.warning(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {doc_id}, ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink() # å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        return False
    except Exception as e:
        logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {doc_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        logging.debug(traceback.format_exc())
        if zip_path.exists():
            zip_path.unlink()
        return False
