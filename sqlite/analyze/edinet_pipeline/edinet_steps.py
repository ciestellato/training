import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm
import warnings
from edinet_config import Config
import logging
import traceback
import sqlite3 # SQLiteã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sqlalchemy.dialects import sqlite
from sqlalchemy import text # text()é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€Core SQLæ“ä½œã‚’è¡Œã†

# zip_utilsã‹ã‚‰CSVæŠ½å‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .zip_utils import extract_csv_from_zip 
from .database_setup import Engine # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ Engine ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from edinet_api import update_summary_file,download_single_file
from . import storage_repo
from . import file_processor

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and " # CSVæœ‰ç„¡ãƒ•ãƒ©ã‚°ãŒ'1' [44]
        "secCode.notna() and secCode != 'None' and " # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}" # å¯¾è±¡æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰ [107]
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("ğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download
    
def log_failed_download(doc_id, submit_date, error_msg):
    """ã‚¹ãƒ†ãƒƒãƒ—3ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ã™ã‚‹"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    failed_record = pd.DataFrame([{
        "docID": doc_id,
        "submitDateTime": submit_date,
        "errorMessage": error_msg,
        "timestamp": timestamp
    }])
    failed_log_path = Config.FAILED_LOG_PATH
    failed_record.to_csv(failed_log_path, mode='a', header=not failed_log_path.exists(), index=False, encoding='utf_8_sig')

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)
        if not success:
            log_failed_download(doc_id, submit_date, "åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")

        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)

def retry_failed_downloads():
    """å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œã—ã€æˆåŠŸã—ãŸã‚‚ã®ã¯ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤ã™ã‚‹"""
    failed_log_path = Config.FAILED_LOG_PATH

    if not failed_log_path.exists():
        logging.info("å†è©¦è¡Œå¯¾è±¡ã®å¤±æ•—ãƒ­ã‚°ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    try:
        failed_df = pd.read_csv(failed_log_path, encoding='utf_8_sig')
    except Exception as e:
        logging.error(f"å¤±æ•—ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return

    if failed_df.empty:
        logging.info("å¤±æ•—ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    logging.info(f"ğŸ” {len(failed_df)} ä»¶ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
    successful_ids = []
    failed_again = [] # å†å¤±æ•—è¨˜éŒ²ç”¨

    for _, row in tqdm(failed_df.iterrows(), total=len(failed_df), desc="å†è©¦è¡Œä¸­"):
        doc_id = row['docID']
        submit_date = pd.to_datetime(row['submitDateTime'], errors='coerce')

        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)

        if success:
            successful_ids.append(doc_id)
        else:
            logging.warning(f"å†è©¦è¡Œå¤±æ•—: {doc_id}")
            failed_again.append((doc_id, submit_date))

        time.sleep(0.1)
        
    # æˆåŠŸã—ãŸIDã‚’ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤
    remaining_df = failed_df[~failed_df['docID'].isin(successful_ids)]
    remaining_df.to_csv(failed_log_path, index=False, encoding='utf_8_sig')

    # å†è¨˜éŒ²ï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã ã‘ï¼‰
    for doc_id, submit_date in failed_again:
        log_failed_download(doc_id, submit_date, "å†è©¦è¡Œå¤±æ•—")

    logging.info(f"âœ… å†è©¦è¡Œå®Œäº†ã€‚æˆåŠŸ: {len(successful_ids)} ä»¶ / æ®‹ã‚Š: {len(remaining_df) + len(failed_again)} ä»¶")

def step5_store_summary_to_db(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘£: å–å¾—ã—ãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘£ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ ---")
    
    # å‡¦ç†ã‚’ storage_repo ã«å§”è­²ã™ã‚‹
    try:
        storage_repo.store_document_summaries(summary_df)
    except Exception:
        # ã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒã‚¸ãƒˆãƒªå´ã§ãƒ­ã‚®ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ãƒ•ãƒ­ãƒ¼ç¶™ç¶š/åœæ­¢ã‚’åˆ¤æ–­
        logging.error("ã‚¹ãƒ†ãƒƒãƒ—â‘£ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        raise
    logging.info("-" * 40)
    
def step6_extract_and_index_csv(zip_base_folder: Path):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¤: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã€ãã®ãƒ‘ã‚¹ã‚’SQLiteã«è¨˜éŒ²ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¤ CSVæŠ½å‡ºã¨æŠ½å‡ºãƒ‘ã‚¹ã®SQLiteè¨˜éŒ² ---")
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å±¤ã«æŠ½å‡ºã¨ã€ãƒªãƒã‚¸ãƒˆãƒªå±¤ã¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚’ä¾é ¼
    file_processor.extract_and_index_all_csvs(zip_base_folder, storage_repo)
    logging.info("-" * 40)

def step7_parse_and_store_csv_data_to_db():
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¦: æŠ½å‡ºã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã€è²¡å‹™æ•°å€¤ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¦ CSVè§£æã¨è²¡å‹™æ•°å€¤ã®SQLiteä¿ç®¡ ---")

    # 1. ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰è§£æå¯¾è±¡ã®CSVãƒ‘ã‚¹ã‚’å–å¾—
    csv_paths_df = file_processor.get_csv_paths_from_repo()
    
    # 2. ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å±¤ã§CSVã‚’è§£æã—ã€æ•´å½¢ã•ã‚ŒãŸDataFrameã‚’å–å¾—
    financial_df = file_processor.parse_all_financial_csvs(csv_paths_df)

    # 3. ãƒªãƒã‚¸ãƒˆãƒªå±¤ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡
    if not financial_df.empty:
        storage_repo.store_financial_data(financial_df)
    logging.info("-" * 40)
