import time
import requests
import pandas as pd
from datetime import date, timedelta, datetime
from pathlib import Path
from tqdm import tqdm
import warnings
import logging
import traceback
from sqlalchemy.dialects import sqlite
from sqlalchemy import text # text()é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€Core SQLæ“ä½œã‚’è¡Œã†

from edinet_config import Config
from .zip_utils import extract_csv_from_zip # zip_utilsã‹ã‚‰CSVæŠ½å‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ 
from .database_setup import Engine # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ Engine ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .database_setup import get_db # DIãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .edinet_models import EdinetExtractedCsvDetails
from edinet_api import update_summary_file,download_single_file
from . import storage_repo  # .pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã¾ã‚‹ã”ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã¨ãã®æ›¸ãæ–¹
from . import file_processor

# urllib3ã®InsecureRequestWarningã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

def step1_create_and_summarize():
    """ã‚¹ãƒ†ãƒƒãƒ—â‘ : ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã€ãã®æ¦‚è¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘  ã‚µãƒãƒªãƒ¼ä½œæˆã¨æ¦‚è¦ã®è¡¨ç¤º ---")
    summary_df = update_summary_file(Config.BASE_DIR, Config.API_KEY)

    if summary_df.empty:
        logging.warning("âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame()

    logging.info(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {summary_df['submitDateTime'].min():%Y-%m-%d} ï½ {summary_df['submitDateTime'].max():%Y-%m-%d}")
    logging.info(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(summary_df)} ä»¶")
    logging.info(f"  - éŠ˜æŸ„æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {summary_df['secCode'].nunique()} ç¤¾")
    logging.info("-" * 40)

    return summary_df

def step2_check_download_status(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ³ã‚’ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä»¶æ•°ãªã©ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã®ç¢ºèª ---")
    save_folder = Config.SAVE_FOLDER
    save_folder.mkdir(parents=True, exist_ok=True)

    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚‚å«ã‚ã¦ã€æ—¢å­˜ã®å…¨zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
    existing_files_path = list(save_folder.rglob('*.zip'))

    logging.info(f"ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€: {save_folder}")
    if not existing_files_path:
        logging.info("  - æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        logging.info(f"  - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files_path)} ä»¶")

    # --- â–¼â–¼â–¼ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–¼â–¼â–¼ ---
    query_str = (
        "csvFlag == '1' and " # CSVæœ‰ç„¡ãƒ•ãƒ©ã‚°ãŒ'1'
        "secCode.notna() and secCode != 'None' and " # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹
        f"docTypeCode in {Config.TARGET_DOC_TYPE_CODES}" # å¯¾è±¡æ›¸é¡ã‚¿ã‚¤ãƒ—ã‚³ãƒ¼ãƒ‰
    )
    target_docs = summary_df.query(query_str)
    # --- â–²â–²â–² ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ¡ä»¶ã‚’å®šç¾© â–²â–²â–² ---

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆdocIDï¼‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    existing_file_stems = {f.stem for f in existing_files_path}

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ã†ã¡ã€ã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    docs_to_download = target_docs[~target_docs['docID'].isin(existing_file_stems)]

    logging.info("ğŸ“Š ã‚µãƒãƒªãƒ¼ã¨ç…§åˆã—ãŸçµæœ:")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ç·æ›¸é¡æ•°ï¼ˆCSVæä¾›ã‚ã‚Šï¼‰: {len(target_docs)} ä»¶")
    logging.info(f"  - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªï¼ˆæœªå–å¾—ã®ï¼‰æ›¸é¡æ•°: {len(docs_to_download)} ä»¶")
    logging.info("-" * 40)

    return docs_to_download
    
def log_failed_download(doc_id, submit_date, error_msg):
    """ã‚¹ãƒ†ãƒƒãƒ—3ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ã™ã‚‹"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    failed_record = pd.DataFrame([{
        "docID": doc_id,
        "submitDateTime": submit_date,
        "errorMessage": error_msg,
        "timestamp": timestamp
    }])
    failed_log_path = Config.FAILED_LOG_PATH
    failed_record.to_csv(failed_log_path, mode='a', header=not failed_log_path.exists(), index=False, encoding='utf_8_sig')

def step3_execute_download(docs_to_download: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¢: å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€å¹´/å››åŠæœŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ---")
    if docs_to_download.empty:
        logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’å®Œäº†ã—ã¾ã™ã€‚")
        logging.info("-" * 40)
        return

    logging.info(f"{len(docs_to_download)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for _, row in tqdm(docs_to_download.iterrows(), total=len(docs_to_download), desc="ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—"):
        doc_id = row['docID']
        submit_date = row['submitDateTime']
        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)
        if not success:
            log_failed_download(doc_id, submit_date, "åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")

        time.sleep(0.1)

    logging.info("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logging.info("-" * 40)

def retry_failed_downloads():
    """å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œã—ã€æˆåŠŸã—ãŸã‚‚ã®ã¯ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤ã™ã‚‹"""
    failed_log_path = Config.FAILED_LOG_PATH

    if not failed_log_path.exists():
        logging.info("å†è©¦è¡Œå¯¾è±¡ã®å¤±æ•—ãƒ­ã‚°ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    try:
        failed_df = pd.read_csv(failed_log_path, encoding='utf_8_sig')
    except Exception as e:
        logging.error(f"å¤±æ•—ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logging.debug(traceback.format_exc())
        return

    if failed_df.empty:
        logging.info("å¤±æ•—ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    logging.info(f"ğŸ” {len(failed_df)} ä»¶ã®å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚")
    successful_ids = []
    failed_again = [] # å†å¤±æ•—è¨˜éŒ²ç”¨

    for _, row in tqdm(failed_df.iterrows(), total=len(failed_df), desc="å†è©¦è¡Œä¸­"):
        doc_id = row['docID']
        submit_date = pd.to_datetime(row['submitDateTime'], errors='coerce')

        success = download_single_file(doc_id, submit_date, Config.SAVE_FOLDER)

        if success:
            successful_ids.append(doc_id)
        else:
            logging.warning(f"å†è©¦è¡Œå¤±æ•—: {doc_id}")
            failed_again.append((doc_id, submit_date))

        time.sleep(0.1)
        
    # æˆåŠŸã—ãŸIDã‚’ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤
    remaining_df = failed_df[~failed_df['docID'].isin(successful_ids)]
    remaining_df.to_csv(failed_log_path, index=False, encoding='utf_8_sig')

    # å†è¨˜éŒ²ï¼ˆå¤±æ•—ã—ãŸã‚‚ã®ã ã‘ï¼‰
    for doc_id, submit_date in failed_again:
        log_failed_download(doc_id, submit_date, "å†è©¦è¡Œå¤±æ•—")

    logging.info(f"âœ… å†è©¦è¡Œå®Œäº†ã€‚æˆåŠŸ: {len(successful_ids)} ä»¶ / æ®‹ã‚Š: {len(remaining_df) + len(failed_again)} ä»¶")

def step5_store_summary_to_db(summary_df: pd.DataFrame):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘£: å–å¾—ã—ãŸã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘£ ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®SQLiteä¿ç®¡ ---")
    
    # å‡¦ç†ã‚’ storage_repo ã«å§”è­²ã™ã‚‹
    try:
        storage_repo.store_document_summaries(summary_df)
    except Exception:
        # ã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒã‚¸ãƒˆãƒªå´ã§ãƒ­ã‚®ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ãƒ•ãƒ­ãƒ¼ç¶™ç¶š/åœæ­¢ã‚’åˆ¤æ–­
        logging.error("ã‚¹ãƒ†ãƒƒãƒ—â‘£ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        raise
    logging.info("-" * 40)
    
def step6_extract_and_index_csv(zip_base_folder: Path):
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¤: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã€ãã®ãƒ‘ã‚¹ã‚’SQLiteã«è¨˜éŒ²ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¤ CSVæŠ½å‡ºã¨æŠ½å‡ºãƒ‘ã‚¹ã®SQLiteè¨˜éŒ² ---")
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—ã™ã‚‹
    # get_db() ã¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãªã®ã§ã€withæ–‡ã§å®‰å…¨ã«åˆ©ç”¨ã™ã‚‹
    with get_db() as db:
        # file_processorå±¤ã«ã€ãƒªãƒã‚¸ãƒˆãƒªå±¤ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¸¡æ–¹ã‚’æ¸¡ã™
        file_processor.extract_and_index_all_csvs(zip_base_folder, storage_repo, db)
        
    logging.info("-" * 40)

def step7_parse_and_store_csv_data_to_db():
    """
    ã‚¹ãƒ†ãƒƒãƒ—â‘¦: æŠ½å‡ºã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã€è²¡å‹™æ•°å€¤ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡ã™ã‚‹ã€‚
    """
    logging.info("--- ã‚¹ãƒ†ãƒƒãƒ—â‘¦ CSVè§£æã¨è²¡å‹™æ•°å€¤ã®SQLiteä¿ç®¡ ---")

    csv_paths_df = pd.DataFrame()

    # 1. ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰è§£æå¯¾è±¡ã®CSVãƒ‘ã‚¹ã‚’å–å¾— (SQLAlchemyã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨)
    try:
        # get_db() ã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®‰å…¨ã«é–‹å§‹
        with get_db() as db: 
            # EdinetExtractedCsvDetails ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã‹ã‚‰ docID ã¨ csv_path ã‚’å–å¾—
            # docID ã¨ csv_path ã¯ ORMãƒ¢ãƒ‡ãƒ« (EdinetExtractedCsvDetails) ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ [3, 4]
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥Pandas DataFrameã«èª­ã¿è¾¼ã‚€
            
            # SQLæ–‡ã‚’å®šç¾©ï¼ˆã“ã“ã§ã¯ã€EdinetExtractedCsvDetailsã‹ã‚‰docIDã¨csv_pathã‚’å–å¾—ï¼‰
            # ORMã®Baseå®šç¾©ãŒdb.bind (Engine)ã«ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€read_sqlãŒä½¿ç”¨å¯èƒ½
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«å: edinet_extracted_csv_details [3]
            sql_query = """
                SELECT 
                    docID, 
                    csv_path AS extracted_path
                FROM 
                    edinet_extracted_csv_details
            """
            
            csv_paths_df = pd.read_sql(sql_query, db.bind)
            
    except Exception as e:
        logging.error(f"CSVãƒ‘ã‚¹ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã“ã“ã§å‡¦ç†ã‚’ä¸­æ–­
        raise 
        
    # 2. ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å±¤ã§CSVã‚’è§£æã—ã€æ•´å½¢ã•ã‚ŒãŸDataFrameã‚’å–å¾—
    financial_df = file_processor.parse_all_financial_csvs(csv_paths_df)

    # 3. ãƒªãƒã‚¸ãƒˆãƒªå±¤ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿ç®¡
    if not financial_df.empty:
        storage_repo.store_financial_data(financial_df)
    
    logging.info("-" * 40)
